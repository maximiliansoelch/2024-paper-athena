@misc{azaiz:2024:FeedbackGenerationProgrammingExercises,
  title = {Feedback-{{Generation}} for {{Programming Exercises With GPT-4}}},
  author = {Azaiz, Imen and Kiesler, Natalie and Strickroth, Sven},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04449},
  eprint = {2403.04449},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-26},
  abstract = {Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/maximiliansoelch/Zotero/storage/ZRVM8E4Q/Azaiz et al. - 2024 - Feedback-Generation for Programming Exercises With.pdf}
}

@article{hahn:2021:SystematicReviewEffects,
  title = {A {{Systematic Review}} of the {{Effects}} of {{Automatic Scoring}} and {{Automatic Feedback}} in {{Educational Settings}}},
  author = {Hahn, Marcelo Guerra and Navarro, Silvia Margarita Baldiris and De La Fuente Valentin, Luis and Burgos, Daniel},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {108190--108198},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3100890},
  urldate = {2024-03-26},
  abstract = {Automatic scoring and feedback tools have become critical components of online learning proliferation. These tools range from multiple-choice questions to grading essays using machine learning (ML). Learning environments such as massive open online courses (MOOCs) would not be possible without them. The usage of this mechanism has brought many exciting areas of study, from the design of questions to the ML grading tools' precision and accuracy. This paper analyzes the findings of 125 studies published in journals and proceedings between 2016 and 2020 on the usages of automatic scoring and feedback as a learning tool. This analysis gives an overview of the trends, challenges, and open questions in this research area. The results indicate that automatic scoring and feedback have many advantages. The most important benefits include enabling scaling the number of students without adding a proportional number of instructors, improving the student experience by reducing the time between submission grading and feedback, and removing bias in scoring. On the other hand, these technologies have some drawbacks. The main problem is creating a disincentive to develop innovative answers that do not match the expected one or have not been considered when preparing the problem. Another drawback is potentially training the student to answer the question instead of learning the concepts. With this, given the existence of a correct answer, such an answer could be leaked to the internet, making it easier for students to avoid solving the problem. Overall, each of these drawbacks presents an opportunity to look at ways to improve technologies to use these tools to provide a better learning experience to students.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/maximiliansoelch/Zotero/storage/SSSSXUHD/Hahn et al. - 2021 - A Systematic Review of the Effects of Automatic Sc.pdf}
}

@book{irons:2007:EnhancingLearningFormative,
  title = {Enhancing {{Learning}} through {{Formative Assessment}} and {{Feedback}}},
  author = {Irons, Alastair},
  year = {2007},
  month = oct,
  publisher = {Routledge},
  address = {London},
  doi = {10.4324/9780203934333},
  abstract = {This book is based on the argument that detailed and developmental formative feedback is the single most useful thing teachers can do for students. It helps to clarify the expectations of higher education and assist all students to achieve their potential.  This book promotes student learning through formative assessment and feedback, which: enables self-assessment and reflection in learning  encourages teacher-student dialogue  helps clarify what is good performance  provides students with quality information to help improve their learning  encourages motivation and self-confidence in students  aids the teacher in shaping teaching  Underpinned by the relevant theory, the practical advice and examples in this book directly address the issues of how to motivate students to engage in formative assessment effectively and shows teachers how they can provide further useful formative feedback.},
  isbn = {978-0-203-93433-3}
}

@article{keuning:2018:SystematicLiteratureReview,
  title = {A Systematic Literature Review of Automated Feedback Generation for Programming Exercises},
  author = {Keuning, Hieke and Jeuring, Johan and Heeren, Bastiaan},
  year = {2018},
  month = sep,
  journal = {ACM Transactions on Computing Education},
  volume = {19},
  number = {1},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3231711},
  abstract = {Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We have performed a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss' feedback content categories to classify feedback messages. We report on the results of coding a total of 101 tools. We have found that feedback mostly focuses on identifying mistakes and less on fixing problems and taking a next step. Furthermore, teachers cannot easily adapt tools to their own needs. However, the diversity of feedback types has increased over the past decades and new techniques are being applied to generate feedback that is increasingly helpful for students.},
  articleno = {3},
  issue_date = {March 2019},
  keywords = {automated feedback,learning programming,programming tools,Systematic literature review},
  file = {/Users/maximiliansoelch/Zotero/storage/C3QGZ93K/Keuning et al. - 2018 - A systematic literature review of automated feedba.pdf}
}

@misc{kiesler:2023:ExploringPotentialLarge,
  title = {Exploring the {{Potential}} of {{Large Language Models}} to {{Generate Formative Programming Feedback}}},
  author = {Kiesler, Natalie and Lohr, Dominic and Keuning, Hieke},
  year = {2023},
  month = aug,
  number = {arXiv:2309.00029},
  eprint = {2309.00029},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-26},
  abstract = {Ever since the emergence of large language models (LLMs) and related applications, such as ChatGPT, its performance and error analysis for programming tasks have been subject to research. In this work-in-progress paper, we explore the potential of such LLMs for computing educators and learners, as we analyze the feedback it generates to a given input containing program code. In particular, we aim at (1) exploring how an LLM like ChatGPT responds to students seeking help with their introductory programming tasks, and (2) identifying feedback types in its responses. To achieve these goals, we used students' programming sequences from a dataset gathered within a CS1 course as input for ChatGPT along with questions required to elicit feedback and correct solutions. The results show that ChatGPT performs reasonably well for some of the introductory programming tasks and student errors, which means that students can potentially benefit. However, educators should provide guidance on how to use the provided feedback, as it can contain misleading information for novices.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Software Engineering},
  file = {/Users/maximiliansoelch/Zotero/storage/XAZ78UA7/Kiesler et al. - 2023 - Exploring the Potential of Large Language Models t.pdf;/Users/maximiliansoelch/Zotero/storage/XY78667R/2309.html}
}

@misc{kiesler:2023:LargeLanguageModels,
  title = {Large {{Language Models}} in {{Introductory Programming Education}}: {{ChatGPT}}'s {{Performance}} and {{Implications}} for {{Assessments}}},
  shorttitle = {Large {{Language Models}} in {{Introductory Programming Education}}},
  author = {Kiesler, Natalie and Schiffner, Daniel},
  year = {2023},
  month = aug,
  number = {arXiv:2308.08572},
  eprint = {2308.08572},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-26},
  abstract = {This paper investigates the performance of the Large Language Models (LLMs) ChatGPT-3.5 and GPT-4 in solving introductory programming tasks. Based on the performance, implications for didactic scenarios and assessment formats utilizing LLMs are derived. For the analysis, 72 Python tasks for novice programmers were selected from the free site CodingBat. Full task descriptions were used as input to the LLMs, while the generated replies were evaluated using CodingBat's unit tests. In addition, the general availability of textual explanations and program code was analyzed. The results show high scores of 94.4 to 95.8\% correct responses and reliable availability of textual explanations and program code, which opens new ways to incorporate LLMs into programming education and assessment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Software Engineering},
  file = {/Users/maximiliansoelch/Zotero/storage/GNMRAEFS/Kiesler and Schiffner - 2023 - Large Language Models in Introductory Programming .pdf;/Users/maximiliansoelch/Zotero/storage/VQHY5ZD9/2308.html}
}

@article{latif:2023:FinetuningChatGPTAutomatic,
  title = {Fine-Tuning {{ChatGPT}} for {{Automatic Scoring}}},
  author = {Latif, Ehsan and Zhai, Xiaoming},
  year = {2023},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2310.10072},
  urldate = {2024-03-26},
  abstract = {This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for automatically scoring student written constructed responses using example assessment tasks in science education. Recent studies on OpenAI's generative model GPT-3.5 proved its superiority in predicting the natural language with high accuracy and human-like responses. GPT-3.5 has been trained over enormous online language materials such as journals and Wikipedia; therefore, more than direct usage of pre-trained GPT-3.5 is required for automatic scoring as students utilize a different language than trained material. These imply that a domain-specific model, fine-tuned over data for specific tasks, can enhance model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks with a diverse dataset of middle-school and high-school student responses and expert scoring. The six tasks comprise two multi-label and four multi-class assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the fine-tuned state-of-the-art Google's generated language model, BERT. The results show that in-domain training corpora constructed from science questions and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5 shows a remarkable average increase (9.1\%) in automatic scoring accuracy (mean = 9.15, SD = 0.042) for the six tasks, p =0.001 \&lt; 0.05. Specifically, for multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5 achieved significantly higher scoring accuracy than BERT across all the labels, with the second item achieving a 7.1\% increase. The average scoring increase for the four multi-class items for GPT-3.5 was 10.6\% compared to BERT. Our study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring of student responses on domain-specific data in education with high accuracy. We have released fine-tuned models for public use and community engagement.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/maximiliansoelch/Zotero/storage/3DMV4VY4/Latif and Zhai - 2023 - Fine-tuning ChatGPT for Automatic Scoring.pdf}
}

@article{lee:2023:ApplyingLargeLanguage,
  title = {Applying {{Large Language Models}} and {{Chain-of-Thought}} for {{Automatic Scoring}}},
  author = {Lee, Gyeong-Geon and Latif, Ehsan and Wu, Xuansheng and Liu, Ninghao and Zhai, Xiaoming},
  year = {2023},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2312.03748},
  urldate = {2024-03-26},
  abstract = {This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6\% increase. CoT, when used without item stem and scoring rubrics, did not significantly affect scoring accuracy (acc = .60). However, CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44\% increase for zero-shot; 3.7\% increase for few-shot). We found a more balanced accuracy across different proficiency categories when CoT was used with a scoring rubric, highlighting the importance of domain-specific reasoning in enhancing the effectiveness of LLMs in scoring tasks. We also found that GPT-4 demonstrated superior performance over GPT -3.5 in various scoring tasks when combined with the single-call greedy sampling or ensemble voting nucleus sampling strategy, showing 8.64\% difference. Particularly, the single-call greedy sampling strategy with GPT-4 outperformed other approaches.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/maximiliansoelch/Zotero/storage/GBJ9LQTK/Lee et al. - 2023 - Applying Large Language Models and Chain-of-Though.pdf}
}

@article{paiva:2022:AutomatedAssessmentComputer,
  title = {Automated {{Assessment}} in {{Computer Science Education}}: {{A State-of-the-Art Review}}},
  shorttitle = {Automated {{Assessment}} in {{Computer Science Education}}},
  author = {Paiva, Jos{\'e} Carlos and Leal, Jos{\'e} Paulo and Figueira, {\'A}lvaro},
  year = {2022},
  month = sep,
  journal = {ACM Transactions on Computing Education},
  volume = {22},
  number = {3},
  pages = {1--40},
  issn = {1946-6226, 1946-6226},
  doi = {10.1145/3513140},
  urldate = {2024-03-26},
  abstract = {Practical programming competencies are critical to the success in computer science (CS) education and go-to-market of fresh graduates. Acquiring the required level of skills is a long journey of discovery, trial and error, and optimization seeking through a broad range of programming activities that learners must perform themselves. It is not reasonable to consider that teachers could evaluate all attempts that the average learner should develop multiplied by the number of students enrolled in a course, much less in a timely, deep, and fair fashion. Unsurprisingly, exploring the formal structure of programs to automate the assessment of certain features has long been a hot topic among CS education practitioners. Assessing a program is considerably more complex than asserting its functional correctness, as the proliferation of tools and techniques in the literature over the past decades indicates. Program efficiency, behavior, and readability, among many other features, assessed either statically or dynamically, are now also relevant for automatic evaluation. The outcome of an evaluation evolved from the primordial Boolean values to information about errors and tips on how to advance, possibly taking into account similar solutions. This work surveys the state of the art in the automated assessment of CS assignments, focusing on the supported types of exercises, security measures adopted, testing techniques used, type of feedback produced, and the information they offer the teacher to understand and optimize learning. A new era of automated assessment, capitalizing on static analysis techniques and containerization, has been identified. Furthermore, this review presents several other findings from the conducted review, discusses the current challenges of the field, and proposes some future research directions.},
  langid = {english},
  file = {/Users/maximiliansoelch/Zotero/storage/Q72ULN47/Paiva et al. - 2022 - Automated Assessment in Computer Science Education.pdf}
}

@article{schartel:2012:GivingFeedbackIntegral,
  title = {Giving Feedback -- {{An}} Integral Part of Education},
  author = {Schartel, Scott A.},
  year = {2012},
  month = mar,
  journal = {Best Practice \& Research Clinical Anaesthesiology},
  series = {Challenges in {{Anaesthesia Education}}},
  volume = {26},
  number = {1},
  pages = {77--87},
  issn = {1521-6896},
  doi = {10.1016/j.bpa.2012.02.003},
  urldate = {2024-06-13},
  abstract = {Feedback is an integral part of the educational process. It provides learners with a comparison of their performance to educational goals with the aim of helping them achieve or exceed their goals. Effective feedback is delivered in an appropriate setting, focusses on performance and not the individual, is specific, is based on direct observation or objective date, is delivered using neutral, non-judgemental language and identifies actions or plans for improvement. For best results, the sender and receiver of feedback must work as allies. Negative feedback can create an emotional response in the learner, which may interfere with the effectiveness of the feedback due to dissonance between self-evaluation and external appraisal. Reflection can help learners process negative feedback and allow them to develop and implement improvement plans. Both delivering and receiving feedback are skills that can be improved with training. Teachers have a duty to provide meaningful feedback to learners; learners should expect feedback and seek it.},
  keywords = {clinical competence,education medical/standards,feedback,knowledge of results (psychology),learning,self-assessment,self-concept,teaching/methods,teaching/trends},
  file = {/Users/maximiliansoelch/Zotero/storage/5TDPGWRJ/S1521689612000067.html}
}

@inproceedings{xue:2024:DoesChatGPTHelpa,
  title = {Does {{ChatGPT}} Help with Introductory {{Programming}}?{{An}} Experiment of Students Using {{ChatGPT}} in {{CS1}}},
  booktitle = {Proceedings of the 46th International Conference on Software Engineering: {{Software}} Engineering Education and Training},
  author = {Xue, Yuankai and Chen, Hanlin and Bai, Gina R. and Tairas, Robert and Huang, Yu},
  year = {2024},
  series = {{{ICSE-SEET}} '24},
  pages = {331--341},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3639474.3640076},
  abstract = {Generative AI, notably ChatGPT, has garnered attention in computer science education. This paper presents a controlled experiment that explores ChatGPT's role in CS1 in a classroom setting. Specifically, we aim to investigate the impact of ChatGPT on student learning outcomes and their behaviors when working on programming assignments. Participants were tasked with creating a UML diagram and subsequently implementing its design through programming, followed by a closed-book post-evaluation and a post-survey. All the participants were required to screen-record the whole process. In total, 56 participants were recruited, with 48 successful screen recordings. Participants in the Experimental Group can access ChatGPT 3.5 and other online resources, such as Google and Stack Overflow when creating the UML diagram and programming; however, participants in the Control Group can access all online resources except for ChatGPT (i.e., the only design variable is the access to ChatGPT). Finally, we measured and analyzed participants' learning outcomes through their UML diagram, programming, and post-evaluation scores. We also analyzed the time participants took to complete the tasks and their interactions with ChatGPT and other resources from the screen recordings. After finishing the tasks, student participants also provided their perceptions of using ChatGPT in CS1 through a post-survey.With rigorous quantitative and qualitative analysis, we found that (1) using ChatGPT does not present a significant impact on students' learning performance in the CS1 assignment-style tasks; (2) once using ChatGPT, students' tendency to explore other traditional educational resources is largely reduced (though available) and they tend to rely solely on ChatGPT, and this reliance on ChatGPT did not guarantee enhanced learning performance; (3) the majority of students hold neutral views on ChatGPT's role in CS1 programming but most of them raised concerns about its potential ethical issues and inconsistent performance across different tasks. We hope this study can help educators and students better understand the impact of ChatGPT in CS1 and inspire future work to provide proper guidelines for using ChatGPT in introductory programming classes.},
  isbn = {9798400704987},
  keywords = {ChatGPT,CS education,CS1,generative AI,OOP},
  file = {/Users/maximiliansoelch/Zotero/storage/KVZJNUHE/Xue et al. - 2024 - Does ChatGPT help with introductory ProgrammingAn.pdf}
}
