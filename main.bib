@inproceedings{agarwal:2022:LEGenTLocalizingErrors,
  title = {{{LEGenT}}: {{Localizing Errors}} and {{Generating Testcases}} for {{CS1}}},
  shorttitle = {{{LEGenT}}},
  author = {Agarwal, Nimisha and Karkare, Amey},
  year = {2022},
  month = jun,
  pages = {102--112},
  doi = {10.1145/3491140.3528282}
}

@inproceedings{aggarwal:2022:UnisonLiveAutomated,
  title = {Unison {{Live}}: {{Automated Feedback}}, {{Grading}}, and {{Analytics LTI Application}}},
  shorttitle = {Unison {{Live}}},
  booktitle = {Proceedings of the 22nd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Aggarwal, Ashish and Vasquez, Aeyzechiah},
  year = {2022},
  month = nov,
  series = {Koli {{Calling}} '22},
  pages = {1},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3564721.3565960},
  urldate = {2024-06-14},
  abstract = {As the enrollments in CS courses continue to increase, the need to grade students' submissions and provide effective feedback promptly at scale is a growing challenge for CS educators. Many autograding solutions have been introduced to address this issue. However, there are multiple barriers to adopting these solutions, including requiring significant changes in a course's workflow, setup processes requiring extensive IT support, and, more importantly, the learning curve for instructors. These inhibit instructors' ability to use autograding solutions effectively. In this demo, we present Unison Live, an automated feedback and grading web application that integrates with LTI (learning-tools-interoperability) compliant learning management systems (LMSs) like Canvas. With its use, instructors can enable autograding instructions on their existing assignments in their CS1/2 courses through an intuitive user interface without changing course specifications. Students submit their program files on the LMS and receive instant feedback and grade reports. Unison Live currently supports programming languages like Python, C++, and MATLAB. After the submission deadline, instructors receive auto-generated code similarity reports and aggregate behavioral analytics on student submissions. We believe that an app like this will not only address the logistical issues related to grading but also pedagogically support the integration of formative \& optional programming assignments that students can practice at their own pace and receive feedback. More details on Unison Live are available on https://www.unisonlive.io/},
  isbn = {978-1-4503-9616-5},
  keywords = {analytics,assessment,autograder,automated feedback,CS1/2,LTI,plagiarism}
}

@article{allen:2007:LikertScalesData,
  title = {Likert {{Scales}} and {{Data Analyses}}},
  author = {Allen, I Elaine and Seaman, Christopher A},
  year = {2007},
  journal = {Quality progress},
  volume = {40},
  number = {7},
  pages = {64--65}
}

@inproceedings{alomar:2024:AutomatingSourceCode,
  title = {Automating {{Source Code Refactoring}} in the {{Classroom}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem and Ouni, Ali},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {60--66},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626252.3630787},
  urldate = {2024-06-15},
  abstract = {Refactoring is the practice of improving software quality without altering its external behavior. Developers intuitively refactor their code for multiple purposes, such as improving program comprehension, reducing code complexity, dealing with technical debt, and removing code smells. However, no prior studies have exposed the students to an experience of the process of antipatterns detection and refactoring correction, and provided students with toolset to practice it. To understand and increase the awareness of refactoring concepts, in this paper, we aim to reflect on our experience with teaching refactoring and how it helps students become more aware of bad programming practices and the importance of correcting them via refactoring. This paper discusses the results of an experiment in the classroom that involved carrying out various refactoring activities for the purpose of removing antipatterns using JDeodorant, an IDE plugin that supports antipatterns detection and refactoring. The results of the quantitative and qualitative analysis with 171 students show that students tend to appreciate the idea of learning refactoring and are satisfied with various aspects of the JDeodorant plugin's operation. Through this experiment, refactoring can turn into a vital part of the computing educational plan. We envision our findings enabling educators to support students with refactoring tools tuned towards safer and trustworthy refactoring.},
  isbn = {9798400704239}
}

@misc{amatriain:2024:PromptDesignEngineering,
  title = {Prompt {{Design}} and {{Engineering}}: {{Introduction}} and {{Advanced Methods}}},
  shorttitle = {Prompt {{Design}} and {{Engineering}}},
  author = {Amatriain, Xavier},
  year = {2024},
  month = may,
  number = {arXiv:2401.14423},
  eprint = {2401.14423},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.14423},
  urldate = {2024-06-28},
  abstract = {Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering}
}

@misc{azaiz:2024:FeedbackGenerationProgrammingExercises,
  title = {Feedback-{{Generation}} for {{Programming Exercises With GPT-4}}},
  author = {Azaiz, Imen and Kiesler, Natalie and Strickroth, Sven},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04449},
  eprint = {2403.04449},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-26},
  abstract = {Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{balse:2023:InvestigatingPotentialGPT3,
  title = {Investigating the {{Potential}} of {{GPT-3}} in {{Providing Feedback}} for {{Programming Assessments}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Innovation}} and {{Technology}} in {{Computer Science Education V}}. 1},
  author = {Balse, Rishabh and Valaboju, Bharath and Singhal, Shreya and Warriem, Jayakrishnan Madathil and Prasad, Prajish},
  year = {2023},
  month = jun,
  series = {{{ITiCSE}} 2023},
  pages = {292--298},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3587102.3588852},
  urldate = {2024-06-24},
  abstract = {Recent advances in artificial intelligence have led to the development of large language models (LLMs), which are able to generate text, images, and source code based on prompts provided by humans. In this paper, we explore the capabilities of an LLM - OpenAI's GPT-3 model to provide feedback for student written code. Specifically, we examine the feasibility of GPT-3 to check, critique and suggest changes to code written by learners in an online programming exam of an undergraduate Python programming course.We collected 1211 student code submissions from 7 questions asked in a programming exam, and provided the GPT-3 model with separate prompts to check, critique and provide suggestions on these submissions. We found that there was a high variability in the accuracy of the model's feedback for student submissions. Across questions, the range for accurately checking the correctness of the code was between 57\% to 79\%, between 41\% to 77\% for accurately critiquing code, and between 32\% and 93\% for suggesting appropriate changes to the code. We also found instances where the model generated incorrect and inconsistent feedback. These findings suggest that models like GPT-3 currently cannot be 'directly' used to provide feedback to students for programming assessments.},
  isbn = {9798400701382}
}

@article{barke:2023:GroundedCopilotHow,
  title = {Grounded {{Copilot}}: {{How Programmers Interact}} with {{Code-Generating Models}}},
  shorttitle = {Grounded {{Copilot}}},
  author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
  year = {2023},
  month = apr,
  journal = {Replication Package for Article: "Grounded Copilot: How Programmers Interact with Code-Generating Models"},
  volume = {7},
  number = {OOPSLA1},
  pages = {78:85--78:111},
  doi = {10.1145/3586030},
  urldate = {2024-06-25},
  abstract = {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants---with a range of prior experience using the assistant---as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.}
}

@inproceedings{becker:2023:ProgrammingHardLeast,
  title = {Programming {{Is Hard}} - {{Or}} at {{Least It Used}} to {{Be}}: {{Educational Opportunities}} and {{Challenges}} of {{AI Code Generation}}},
  shorttitle = {Programming {{Is Hard}} - {{Or}} at {{Least It Used}} to {{Be}}},
  booktitle = {Proceedings of the 54th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Becker, Brett A. and Denny, Paul and {Finnie-Ansley}, James and {Luxton-Reilly}, Andrew and Prather, James and Santos, Eddie Antonio},
  year = {2023},
  month = mar,
  series = {{{SIGCSE}} 2023},
  pages = {500--506},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3545945.3569759},
  urldate = {2024-06-25},
  abstract = {The introductory programming sequence has been the focus of much research in computing education. The recent advent of several viable and freely-available AI-driven code generation tools present several immediate opportunities and challenges in this domain. In this position paper we argue that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on overcoming otherwise mitigating the possible challenges. Assuming that the effectiveness and proliferation of these tools will continue to progress rapidly, without quick, deliberate, and concerted efforts, educators will lose advantage in helping shape what opportunities come to be, and what challenges will endure. With this paper we aim to seed this discussion within the computing education community.},
  isbn = {978-1-4503-9431-4}
}

@inproceedings{birillo:2022:HyperstyleToolAssessing,
  title = {Hyperstyle: {{A Tool}} for {{Assessing}} the {{Code Quality}} of {{Solutions}} to {{Programming Assignments}}},
  shorttitle = {Hyperstyle},
  booktitle = {Proceedings of the 53rd {{ACM Technical Symposium}} on {{Computer Science Education}} - {{Volume}} 1},
  author = {Birillo, Anastasiia and Vlasov, Ilya and Burylov, Artyom and Selishchev, Vitalii and Goncharov, Artyom and Tikhomirova, Elena and Vyahhi, Nikolay and Bryksin, Timofey},
  year = {2022},
  month = feb,
  series = {{{SIGCSE}} 2022},
  volume = {1},
  pages = {307--313},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3478431.3499294},
  urldate = {2024-06-16},
  abstract = {In software engineering, it is not enough to simply write code that only works as intended, even if it is free from vulnerabilities and bugs. Every programming language has a style guide and a set of best practices defined by its community, which help practitioners to build solutions that have a clear structure and therefore are easy to read and maintain. To introduce assessment of code quality into the educational process, we developed a tool called Hyperstyle. To make it reflect the needs of the programming community and at the same time be easily extendable, we built it upon several existing professional linters and code checkers. Hyperstyle supports four programming languages (Python, Java, Kotlin, and Javascript) and can be used as a standalone tool or integrated into a MOOC platform. We have integrated the tool into two educational platforms, Stepik and JetBrains Academy, and it has been used to process about one million submissions every week since May 2021.},
  isbn = {978-1-4503-9070-5},
  keywords = {code formatting,code quality assessment,learning programming,programming education,refactoring}
}

@article{bucinca:2021:TrustThinkCognitive,
  title = {To {{Trust}} or to {{Think}}: {{Cognitive Forcing Functions Can Reduce Overreliance}} on {{AI}} in {{AI-assisted Decision-making}}},
  shorttitle = {To {{Trust}} or to {{Think}}},
  author = {Bu{\c c}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z.},
  year = {2021},
  month = apr,
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {5},
  number = {CSCW1},
  pages = {188:1--188:21},
  doi = {10.1145/3449287},
  urldate = {2024-06-25},
  abstract = {People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.}
}

@inproceedings{charitsis:2022:UsingNLPQuantify,
  title = {Using {{NLP}} to {{Quantify Program Decomposition}} in {{CS1}}},
  booktitle = {Proceedings of the {{Ninth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Charitsis, Charis and Piech, Chris and Mitchell, John C.},
  year = {2022},
  month = jun,
  series = {L@{{S}} '22},
  pages = {113--120},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3491140.3528272},
  urldate = {2024-06-14},
  abstract = {Decomposition is a problem-solving technique that is essential to software development. Nonetheless, it is perceived as the most challenging programming skill for learners to master. Researchers have studied decomposition in introductory programming courses through guided experiments, case studies, and surveys. We believe that the rapid advancements in scientific fields such as machine learning and natural language processing (NLP) opened up opportunities for more scalable approaches. We study the relationship between problem-related entities and functional decomposition. We use an automated system to collect 78,500 code snapshots from two CS1 programming assignments of 250 students and then apply NLP techniques to quantify the learner's ability to break down a problem into a series of smaller, more straightforward tasks. We compare different behaviors and evaluate at scale the impact of decomposition on the time it takes to deliver the solution, its complexity, and the student's performance in the assignment and the course exams. Finally, we discuss the implications of our results for teaching and future research.},
  isbn = {978-1-4503-9158-0},
  keywords = {introductory programming courses,problem decomposition,problem-solving approaches,software development}
}

@inproceedings{chen:2024:EnhancingAutomatedFeedbacka,
  title = {Enhancing {{Automated Feedback}} in {{On-Going Assignments}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 2},
  author = {Chen, Huanyi and Ward, Paul A.S.},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {1596--1597},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626253.3635571},
  urldate = {2024-06-15},
  abstract = {In programming courses, test-based automated feedback systems often face a limitation: instructors cannot effectively enhance feedback during ongoing assignments. While a passing test case may adequately indicate that certain rubric criteria have been met by a student's program, failure can arise from many reasons. When a test case fails due to reasons not previously coded for, it can create confusion. This tends to divert a student's focus from genuine learning towards guessing the test. A more effective approach would be for the system to notify instructors when a test fails, rather than automatically returning a "test case failed'' message. Instructors could then diagnose the cause to provide an initial ''human-in-the-loop'' feedback. Subsequently, this feedback can be coded into the test suite. This method enables the improvement of feedback during an on-going assignment. In this poster, we propose an approach to achieve this objective and introduce a supporting tool.},
  isbn = {9798400704246},
  keywords = {automated feedback,computing education,human-in-the-loop feedback}
}

@article{dawson:2019:WhatMakesEffective,
  title = {What Makes for Effective Feedback: Staff and Student Perspectives},
  shorttitle = {What Makes for Effective Feedback},
  author = {Dawson, Phillip and Henderson, Michael and Mahoney, Paige and Phillips, Michael and Ryan, Tracii and Boud, David and Molloy, Elizabeth},
  year = {2019},
  month = jan,
  journal = {Assessment \& Evaluation in Higher Education},
  volume = {44},
  number = {1},
  pages = {25--36},
  publisher = {Routledge},
  issn = {0260-2938},
  doi = {10.1080/02602938.2018.1467877},
  urldate = {2024-06-25},
  abstract = {Since the early 2010s the literature has shifted to view feedback as a process that students do where they make sense of information about work they have done, and use it to improve the quality of their subsequent work. In this view, effective feedback needs to demonstrate effects. However, it is unclear if educators and students share this understanding of feedback. This paper reports a qualitative investigation of what educators and students think the purpose of feedback is, and what they think makes feedback effective. We administered a survey on feedback that was completed by 406 staff and 4514 students from two Australian universities. Inductive thematic analysis was conducted on data from a sample of 323 staff with assessment responsibilities and 400 students. Staff and students largely thought the purpose of feedback was improvement. With respect to what makes feedback effective, staff mostly discussed feedback design matters like timing, modalities and connected tasks. In contrast, students mostly wrote that high-quality feedback comments make feedback effective -- especially comments that are usable, detailed, considerate of affect and personalised to the student's own work. This study may assist researchers, educators and academic developers in refocusing their efforts in improving feedback.},
  keywords = {Assessment feedback,effective feedback,purpose of feedback}
}

@article{dawson:2019:WhatMakesEffectivea,
  title = {What Makes for Effective Feedback: Staff and Student Perspectives},
  shorttitle = {What Makes for Effective Feedback},
  author = {Dawson, Phillip and Henderson, Michael and Mahoney, Paige and Phillips, Michael and Ryan, Tracii and Boud, David and Molloy, Elizabeth},
  year = {2019},
  month = jan,
  journal = {Assessment \& Evaluation in Higher Education},
  volume = {44},
  number = {1},
  pages = {25--36},
  publisher = {Routledge},
  issn = {0260-2938},
  doi = {10.1080/02602938.2018.1467877},
  urldate = {2024-06-26},
  abstract = {Since the early 2010s the literature has shifted to view feedback as a process that students do where they make sense of information about work they have done, and use it to improve the quality of their subsequent work. In this view, effective feedback needs to demonstrate effects. However, it is unclear if educators and students share this understanding of feedback. This paper reports a qualitative investigation of what educators and students think the purpose of feedback is, and what they think makes feedback effective. We administered a survey on feedback that was completed by 406 staff and 4514 students from two Australian universities. Inductive thematic analysis was conducted on data from a sample of 323 staff with assessment responsibilities and 400 students. Staff and students largely thought the purpose of feedback was improvement. With respect to what makes feedback effective, staff mostly discussed feedback design matters like timing, modalities and connected tasks. In contrast, students mostly wrote that high-quality feedback comments make feedback effective -- especially comments that are usable, detailed, considerate of affect and personalised to the student's own work. This study may assist researchers, educators and academic developers in refocusing their efforts in improving feedback.},
  keywords = {Assessment feedback,effective feedback,purpose of feedback}
}

@article{deeva:2021:ReviewAutomatedFeedback,
  title = {A Review of Automated Feedback Systems for Learners: {{Classification}} Framework, Challenges and Opportunities},
  shorttitle = {A Review of Automated Feedback Systems for Learners},
  author = {Deeva, Galina and Bogdanova, Daria and Serral, Estefan{\'i}a and Snoeck, Monique and De Weerdt, Jochen},
  year = {2021},
  month = mar,
  journal = {Computers \& Education},
  volume = {162},
  pages = {104094},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2020.104094},
  urldate = {2024-06-25},
  abstract = {Teacher feedback provided to learners in real-time is a crucial factor for their knowledge and skills acquisition. However, providing real-time feedback at an individual level is often infeasible, considering limited teaching resources. Fortunately, recent technological advancements have allowed for developing of various computer tutoring systems, which can support learners at any place and time by generating personalized feedback automatically. Such systems have emerged in various domains, tackle different educational tasks, and often are designed in very distinctive ways. Consequently, the knowledge in the field of automated feedback systems is rather scattered across different domains and applications, as we illustrate in this study. This paper aims to outline the state-of-the-art of recently developed systems for delivering automated feedback, and thus serves as a source of systematic information for educators, researchers in the educational domain and system developers. More specifically, the contribution of this study is twofold. Firstly, we offer an extensive literature review of the field. As a result of a rigorous selection process, consisting of 4 phases, a total of 109 automated feedback systems is selected for a detailed review and thoroughly classified against numerous dimensions. Secondly, based on a detailed analysis of the recent literature sources and following the design science research approach, a classification framework for automated feedback systems is developed. This framework is used to classify the selected systems, and thus give a detailed overview of the predominantly available educational technologies, the educational settings in which they are applied, the properties of automated feedback they deliver, and the approaches for their design and evaluation. Based on this analysis, several important observations and recommendations are put forward as an outcome of the study. In particular, our study outlines the current fragmentation of the field, discusses a need for a common reference framework, and calls for more personalized, data-driven and student-centered solutions to exploit a larger set of opportunities offered in this age of data.},
  keywords = {Automated feedback,Computer tutoring,e-learning,Intelligent tutoring systems,Systematic literature review,Technology-enhanced learning}
}

@inproceedings{demszky:2023:MPoweringTeachersNatural,
  title = {M-{{Powering Teachers}}: {{Natural Language Processing Powered Feedback Improves}} 1:1 {{Instruction}} and {{Student Outcomes}}},
  shorttitle = {M-{{Powering Teachers}}},
  booktitle = {Proceedings of the {{Tenth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Demszky, Dorottya and Liu, Jing},
  year = {2023},
  month = jul,
  series = {L@{{S}} '23},
  pages = {59--69},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3573051.3593379},
  urldate = {2024-06-14},
  abstract = {Although learners are being connected 1:1 with instructors at an increasing scale, most of these instructors do not receive effective, consistent feedback to help them improve. We deployed M-Powering Teachers, an automated tool based on natural language processing to give instructors feedback on dialogic instructional practices ---including their uptake of student contributions, talk time and questioning practices --- in a 1:1 online learning context. We conducted a randomized controlled trial on Polygence, a research mentorship platform for high schoolers (n=414 mentors) to evaluate the effectiveness of the feedback tool. We find that the intervention improved mentors' uptake of student contributions by 10\%, reduced their talk time by 5\% and improved student's experience with the program as well as their relative optimism about their academic future. These results corroborate existing evidence that scalable and low-cost automated feedback can improve instruction and learning in online educational contexts.},
  isbn = {9798400700255},
  keywords = {automated teacher feedback,natural language processing,randomized controlled trial}
}

@inproceedings{deriba:2024:EnhancingComputerProgramming,
  title = {Enhancing {{Computer Programming Education}} Using {{ChatGPT- A Mini Review}}},
  booktitle = {Proceedings of the 23rd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Deriba, Fitsum Gizachew and Sanusi, Ismaila Temitayo and Sunday, Amos Oyelere},
  year = {2024},
  month = feb,
  series = {Koli {{Calling}} '23},
  pages = {1--2},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3631802.3631848},
  urldate = {2024-06-14},
  abstract = {This paper aims to provide insights into how ChatGPT enhances computer programming education by synthesizing existing studies using rapid review. We analysed 13 articles published in 2023, where studies focused on different aspects of basic programming education. The results indicate that 21\% of these studies demonstrate that ChatGPT served as a tool for code explanation and handling complex topics. However, 36\% show that ChatGPT had difficulty answering non-text-based and code-related questions, revealing reliability and accuracy issues with these tools. Another 36\% of the studies showed that blindly over-reliance on ChatGPT affected critical thinking, student creativity, and problem-solving skills in programming education. 46\% of the studies indicated the need to provide clear guidelines and employ plagiarism-detection tools to instruct students effectively. We suggest that educators should adopt diverse approaches to integrating ChatGPT as an educational tool while highlighting ethical considerations and model limitations.},
  isbn = {9798400716539}
}

@inproceedings{elhayany:2023:AutomatedCodeAssessment,
  title = {Towards {{Automated Code Assessment}} with {{OpenJupyter}} in {{MOOCs}}},
  booktitle = {Proceedings of the {{Tenth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Elhayany, Mohamed and Meinel, Christoph},
  year = {2023},
  month = jul,
  series = {L@{{S}} '23},
  pages = {321--325},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3573051.3596180},
  urldate = {2024-06-14},
  abstract = {The popularity of Massive Open Online Courses (MOOCs) as a means of delivering education to large numbers of students has been growing steadily over the last decade. As technology improves, more educational content is becoming readily available to the public. JupyterLab, an open-source web-based interactive development environment (IDE), is also becoming increasingly popular in education, however, it is so far primarily used in small classroom settings. JupyterLab can provide a more interactive, hands-on, and collaborative learning experience for students in MOOCs, and it is highly customizable and can be accessed from anywhere. To capitalize on these benefits, we have developed OpenJupyter, which integrates JupyterLab at scale with MOOCs, enhancing the student learning experience and providing hands-on exercises for data science courses, making them more interactive and engaging. While MOOCs provide access to education for a large number of students, one of the significant challenges is providing effective and timely feedback to learners. OpenJupyter includes an auto-assessment capability that addresses this problem in MOOCs by automating the evaluation process and providing feedback to learners in a timely manner. In this paper, we provide an overview of the architecture of OpenJupyter, its scalability in the context of MOOCs, and its effectiveness in addressing the auto-assessment challenge. We also discuss the Advantages and limitations associated with using OpenJupyter in a MOOC context and provide a reference for educators and researchers who wish to implement similar tools. Our efforts aim to foster an open educational environment in the field of programming by providing learners with an interactive learning tool and a streamlined technical setup, allowing them to acquire and test their knowledge at their own pace.},
  isbn = {9798400700255},
  keywords = {auto-assessment,JupyterLab,MOOC,openjupyter,programming}
}

@misc{gao:2024:RetrievalAugmentedGenerationLarge,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  year = {2024},
  month = mar,
  number = {arXiv:2312.10997},
  eprint = {2312.10997},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10997},
  urldate = {2024-06-28},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{hahn:2021:SystematicReviewEffects,
  title = {A {{Systematic Review}} of the {{Effects}} of {{Automatic Scoring}} and {{Automatic Feedback}} in {{Educational Settings}}},
  author = {Hahn, Marcelo Guerra and Navarro, Silvia Margarita Baldiris and De La Fuente Valentin, Luis and Burgos, Daniel},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {108190--108198},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3100890},
  urldate = {2024-03-26},
  abstract = {Automatic scoring and feedback tools have become critical components of online learning proliferation. These tools range from multiple-choice questions to grading essays using machine learning (ML). Learning environments such as massive open online courses (MOOCs) would not be possible without them. The usage of this mechanism has brought many exciting areas of study, from the design of questions to the ML grading tools' precision and accuracy. This paper analyzes the findings of 125 studies published in journals and proceedings between 2016 and 2020 on the usages of automatic scoring and feedback as a learning tool. This analysis gives an overview of the trends, challenges, and open questions in this research area. The results indicate that automatic scoring and feedback have many advantages. The most important benefits include enabling scaling the number of students without adding a proportional number of instructors, improving the student experience by reducing the time between submission grading and feedback, and removing bias in scoring. On the other hand, these technologies have some drawbacks. The main problem is creating a disincentive to develop innovative answers that do not match the expected one or have not been considered when preparing the problem. Another drawback is potentially training the student to answer the question instead of learning the concepts. With this, given the existence of a correct answer, such an answer could be leaked to the internet, making it easier for students to avoid solving the problem. Overall, each of these drawbacks presents an opportunity to look at ways to improve technologies to use these tools to provide a better learning experience to students.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english}
}

@inproceedings{hellas:2023:ExploringResponsesLargea,
  title = {Exploring the {{Responses}} of {{Large Language Models}} to {{Beginner Programmers}}' {{Help Requests}}},
  booktitle = {Proceedings of the 2023 {{ACM Conference}} on {{International Computing Education Research}} - {{Volume}} 1},
  author = {Hellas, Arto and Leinonen, Juho and Sarsa, Sami and Koutcheme, Charles and Kujanp{\"a}{\"a}, Lilja and Sorva, Juha},
  year = {2023},
  month = sep,
  series = {{{ICER}} '23},
  volume = {1},
  pages = {93--105},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3568813.3600139},
  urldate = {2024-06-18},
  abstract = {Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers' help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students' code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90\% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57\% of the time). False positives are common (40\% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.},
  isbn = {978-1-4503-9976-0},
  keywords = {automatic feedback,CS1,GPT,help seeking,introductory programming education,large language models,OpenAI Codex,student questions}
}

@article{henderson:2019:ChallengesFeedbackHigher,
  title = {The Challenges of Feedback in Higher Education},
  author = {Henderson, Michael and Ryan, Tracii and Phillips, Michael},
  year = {2019},
  month = nov,
  journal = {Assessment \& Evaluation in Higher Education},
  volume = {44},
  number = {8},
  pages = {1237--1252},
  publisher = {Routledge},
  issn = {0260-2938},
  doi = {10.1080/02602938.2019.1599815},
  urldate = {2024-06-26},
  abstract = {Assessment feedback is one of the most important components of the learning process. However, student and educator dissatisfaction with feedback practices continues to remain a significant problem in higher education. To better understand the barriers to effective feedback, the present study explores feedback challenges identified by 3807 students and 281 educators from two Australian universities. Open-response data were analysed using an inductively derived coding framework and thematic analysis. The findings reveal three major themes relating to the challenges of feedback from the perspective of students and educators: feedback practices, contextual constraints and individual capacity. In addition to confirming the persistence of many previously reported challenges in the literature, this paper also offers insights into challenges that are particular to feedback, especially relating to the production of useful comments and the perceived hurdles of both student and educator attitudes and capabilities. The findings also highlight the constrained nature of educators' work, particularly in relation to time. While further research is needed, we propose that feedback needs to be understood as an interaction between practices, context and individuals.},
  keywords = {attitude,capacity,challenges with feedback,feedback design}
}

@article{higgins:2002:ConscientiousConsumerReconsidering,
  title = {The {{Conscientious Consumer}}: {{Reconsidering}} the Role of Assessment Feedback in Student Learning},
  shorttitle = {The {{Conscientious Consumer}}},
  author = {Higgins, Richard and Hartley, Peter and Skelton, Alan},
  year = {2002},
  month = feb,
  journal = {Studies in Higher Education},
  volume = {27},
  number = {1},
  pages = {53--64},
  publisher = {Routledge},
  issn = {0307-5079},
  doi = {10.1080/03075070120099368},
  urldate = {2024-06-26},
  abstract = {This article reports the initial findings of a 3-year research project investigating the meaning and impact of assessment feedback for students in higher education. Adopting aspects of a constructivist theory of learning, it is seen that formative assessment feedback is essential to encourage the kind of 'deep' learning desired by tutors. There are a number of barriers to the utility of feedback outside the sphere of control of individual students, including those relating to the quality, quantity and language of comments. But the students in the study seemed to read and value their tutors' comments. Their perceptions of feedback do not indicate that they are simply instrumental 'consumers' of education, driven solely by the extrinsic motivation of the mark and as such desire feedback which simply provides them with 'correct answers'. Rather, the situation is more complex. While recognising the importance of grades, many of the students in the study adopt a more 'conscientious' approach. They are motivated intrinsically and seek feedback which will help them to engage with their subject in a 'deep' way. Implications of the findings for theory and practice are discussed.}
}

@misc{huang:2023:SurveyHallucinationLargea,
  title = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}: {{Principles}}, {{Taxonomy}}, {{Challenges}}, and {{Open Questions}}},
  shorttitle = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}},
  author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05232},
  eprint = {2311.05232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.05232},
  urldate = {2024-06-27},
  abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{huang:2023:SurveyHallucinationLargeb,
  title = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}: {{Principles}}, {{Taxonomy}}, {{Challenges}}, and {{Open Questions}}},
  shorttitle = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}},
  author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05232},
  eprint = {2311.05232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.05232},
  urldate = {2024-06-28},
  abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@book{irons:2007:EnhancingLearningFormative,
  title = {Enhancing {{Learning}} through {{Formative Assessment}} and {{Feedback}}},
  author = {Irons, Alastair},
  year = {2007},
  month = oct,
  publisher = {Routledge},
  address = {London},
  doi = {10.4324/9780203934333},
  abstract = {This book is based on the argument that detailed and developmental formative feedback is the single most useful thing teachers can do for students. It helps to clarify the expectations of higher education and assist all students to achieve their potential.  This book promotes student learning through formative assessment and feedback, which: enables self-assessment and reflection in learning  encourages teacher-student dialogue  helps clarify what is good performance  provides students with quality information to help improve their learning  encourages motivation and self-confidence in students  aids the teacher in shaping teaching  Underpinned by the relevant theory, the practical advice and examples in this book directly address the issues of how to motivate students to engage in formative assessment effectively and shows teachers how they can provide further useful formative feedback.},
  isbn = {978-0-203-93433-3}
}

@inproceedings{jamal:2024:EnhancingFormativeFeedback,
  title = {Enhancing {{Formative Feedback}} at {{Scale}} with the {{Intelligent Feedback Assistant}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 2},
  author = {Jamal, Rifa and Renzella, Jake},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {1692--1693},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626253.3635482},
  urldate = {2024-06-15},
  abstract = {Formative feedback spans various domains, from education to businesses and creative endeavours. In educational contexts, feedback enriches students' learning and work quality through reflection. However, providing effective feedback at scale is challenging. Students struggle to engage with feedback, often due to lack of feedback literacy. Recent advancements in Natural Language Processing, a branch of Artificial Intelligence, provides opportunities to evaluate how we can support feedback providers in its quality and scale. This poster paper presents an overview of key feedback challenges, attributes of high quality feedback, and introduces the Intelligent Feedback Assistant (IFA), an innovative NLP-based system designed to assist educators in delivering high-quality feedback. IFA operates as an ensemble of machine learning models and non-AI systems to guide educators in refining their feedback, ensuring it embodies attributes of effective feedback - actionable, specific, justified, and positive. IFA is supportive, not generative, ensuring the feedback provider remains central to the feedback provision process. The tool design, and outcomes of IFA offers a promising path for scaleable, high-quality formative feedback in education and beyond.},
  isbn = {9798400704246},
  keywords = {ai in education,natural language processing,student feedback}
}

@article{jonsson:2013:FacilitatingProductiveUse,
  title = {Facilitating Productive Use of Feedback in Higher Education},
  author = {Jonsson, Anders},
  year = {2013},
  journal = {Active Learning in Higher Education},
  volume = {14},
  number = {1},
  pages = {63--76},
  publisher = {Sage Publications},
  address = {US},
  issn = {1741-2625},
  doi = {10.1177/1469787412467125},
  abstract = {Although feedback has a great potential for learning, students do not always make use of this potential. This article therefore reviews research literature on students' use of feedback in higher education. This is done in order to find answers as to why some students do not use the feedback they receive and which factors are important in influencing students' use of teacher feedback. Findings show that utility is not only a key feature for students' use of feedback but also that some factors, such as lack of strategies for productively using feedback or lack of understanding of academic discourse, may hinder students' possibilities to use the information formatively. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Feedback,Higher Education,School Learning,Student Characteristics}
}

@inproceedings{kazemitabaar:2023:StudyingEffectAIa,
  title = {Studying the Effect of {{AI Code Generators}} on {{Supporting Novice Learners}} in {{Introductory Programming}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kazemitabaar, Majeed and Chow, Justin and Ma, Carl Ka To and Ericson, Barbara J. and Weintrop, David and Grossman, Tovi},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--23},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3580919},
  urldate = {2024-06-25},
  abstract = {AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modification tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex.},
  isbn = {978-1-4503-9421-5}
}

@inproceedings{kazemitabaar:2024:HowNovicesUse,
  title = {How {{Novices Use LLM-based Code Generators}} to {{Solve CS1 Coding Tasks}} in a {{Self-Paced Learning Environment}}},
  booktitle = {Proceedings of the 23rd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Kazemitabaar, Majeed and Hou, Xinying and Henley, Austin and Ericson, Barbara Jane and Weintrop, David and Grossman, Tovi},
  year = {2024},
  month = feb,
  series = {Koli {{Calling}} '23},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3631802.3631806},
  urldate = {2024-06-13},
  abstract = {As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners' utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners' use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.},
  isbn = {9798400716539},
  keywords = {ChatGPT,Copilot,Introductory Programming,Large Language Models,OpenAI Codex,Self-paced Learning,Self-regulation}
}

@article{keuning:2018:SystematicLiteratureReview,
  title = {A Systematic Literature Review of Automated Feedback Generation for Programming Exercises},
  author = {Keuning, Hieke and Jeuring, Johan and Heeren, Bastiaan},
  year = {2018},
  month = sep,
  journal = {ACM Transactions on Computing Education},
  volume = {19},
  number = {1},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3231711},
  abstract = {Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We have performed a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss' feedback content categories to classify feedback messages. We report on the results of coding a total of 101 tools. We have found that feedback mostly focuses on identifying mistakes and less on fixing problems and taking a next step. Furthermore, teachers cannot easily adapt tools to their own needs. However, the diversity of feedback types has increased over the past decades and new techniques are being applied to generate feedback that is increasingly helpful for students.},
  articleno = {3},
  issue_date = {March 2019},
  keywords = {automated feedback,learning programming,programming tools,Systematic literature review}
}

@misc{kiesler:2023:ExploringPotentialLarge,
  title = {Exploring the {{Potential}} of {{Large Language Models}} to {{Generate Formative Programming Feedback}}},
  author = {Kiesler, Natalie and Lohr, Dominic and Keuning, Hieke},
  year = {2023},
  month = aug,
  number = {arXiv:2309.00029},
  eprint = {2309.00029},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-26},
  abstract = {Ever since the emergence of large language models (LLMs) and related applications, such as ChatGPT, its performance and error analysis for programming tasks have been subject to research. In this work-in-progress paper, we explore the potential of such LLMs for computing educators and learners, as we analyze the feedback it generates to a given input containing program code. In particular, we aim at (1) exploring how an LLM like ChatGPT responds to students seeking help with their introductory programming tasks, and (2) identifying feedback types in its responses. To achieve these goals, we used students' programming sequences from a dataset gathered within a CS1 course as input for ChatGPT along with questions required to elicit feedback and correct solutions. The results show that ChatGPT performs reasonably well for some of the introductory programming tasks and student errors, which means that students can potentially benefit. However, educators should provide guidance on how to use the provided feedback, as it can contain misleading information for novices.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Software Engineering}
}

@misc{kiesler:2023:LargeLanguageModels,
  title = {Large {{Language Models}} in {{Introductory Programming Education}}: {{ChatGPT}}'s {{Performance}} and {{Implications}} for {{Assessments}}},
  shorttitle = {Large {{Language Models}} in {{Introductory Programming Education}}},
  author = {Kiesler, Natalie and Schiffner, Daniel},
  year = {2023},
  month = aug,
  number = {arXiv:2308.08572},
  eprint = {2308.08572},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-26},
  abstract = {This paper investigates the performance of the Large Language Models (LLMs) ChatGPT-3.5 and GPT-4 in solving introductory programming tasks. Based on the performance, implications for didactic scenarios and assessment formats utilizing LLMs are derived. For the analysis, 72 Python tasks for novice programmers were selected from the free site CodingBat. Full task descriptions were used as input to the LLMs, while the generated replies were evaluated using CodingBat's unit tests. In addition, the general availability of textual explanations and program code was analyzed. The results show high scores of 94.4 to 95.8\% correct responses and reliable availability of textual explanations and program code, which opens new ways to incorporate LLMs into programming education and assessment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Software Engineering}
}

@article{kirschner:2006:WhyMinimalGuidance,
  title = {Why {{Minimal Guidance During Instruction Does Not Work}}: {{An Analysis}} of the {{Failure}} of {{Constructivist}}, {{Discovery}}, {{Problem-Based}}, {{Experiential}}, and {{Inquiry-Based Teaching}}},
  shorttitle = {Why {{Minimal Guidance During Instruction Does Not Work}}},
  author = {Kirschner, Paul A. and Sweller, John and Clark, Richard E.},
  year = {2006},
  month = jun,
  journal = {Educational Psychologist},
  volume = {41},
  number = {2},
  pages = {75--86},
  publisher = {Routledge},
  issn = {0046-1520},
  doi = {10.1207/s15326985ep4102_1},
  urldate = {2024-06-27},
  abstract = {Evidence for the superiority of guided instruction is explained in the context of our knowledge of human cognitive architecture, expert--novice differences, and cognitive load. Although unguided or minimally guided instructional approaches are very popular and intuitively appealing, the point is made that these approaches ignore both the structures that constitute human cognitive architecture and evidence from empirical studies over the past half-century that consistently indicate that minimally guided instruction is less effective and less efficient than instructional approaches that place a strong emphasis on guidance of the student learning process. The advantage of guidance begins to recede only when learners have sufficiently high prior knowledge to provide "internal" guidance. Recent developments in instructional research and instructional design models that support guidance during instruction are briefly described.}
}

@inproceedings{kok:2023:CombGivingFeedback,
  title = {Comb: {{Giving Feedback}} to {{Short Answer}} at {{Scale}} with {{Human-in-the-Loop Rubric Creation}}},
  shorttitle = {Comb},
  booktitle = {Proceedings of the {{Tenth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Kok, Christopher and Wang, Xu},
  year = {2023},
  month = jul,
  series = {L@{{S}} '23},
  pages = {398--400},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3573051.3596195},
  urldate = {2024-06-14},
  abstract = {As enrollment in college classes rises, it is increasingly difficult to grade and provide feedback to open-ended assignments. It is a timeconsuming and labor-intensive task for instructors, especially when the grading criteria are subjective and constantly evolving. Rubrics are often used to standardize grading, but they can be challenging to create and may not always capture the nuances of a particular assignment. Additionally, it can be difficult to articulate principles or constraints that define a "good" solution in less well-defined domains; like human-computer interaction (HCI) [3, 8] or user experience (UX) [7]. Instructors may delegate the task of grading and offering feedback to a number of graders. However, through our co-design study (section 2.1), we've found that inter-grader reliability, managing time constraints, and dealing with unclear rubrics are just some of the many issues faced by graders in this process.},
  isbn = {9798400700255},
  keywords = {applied natural language processing,automatic feedback,educational technology,human-in-the-loop,short answer grading}
}

@inproceedings{koutcheme:2022:OpenNaturalLanguage,
  title = {Towards {{Open Natural Language Feedback Generation}} for {{Novice Programmers}} Using {{Large Language Models}}},
  booktitle = {Proceedings of the 22nd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Koutcheme, Charles},
  year = {2022},
  month = nov,
  series = {Koli {{Calling}} '22},
  pages = {1--2},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3564721.3565955},
  urldate = {2024-06-14},
  abstract = {Automated feedback on programming exercises has traditionally focused on correctness of submitted exercises. The correctness has been inferred, for example, based on a set of unit tests. Recent advances in the area of providing feedback have suggested relying on large language models for building feedback. In this poster, we present an approach for automatically constructed formative feedback, written in natural language, that builds on two streams of research: (1) automatic program repair, and (2) automatically generating descriptions of programs. Building on combining these two streams, we propose a new approach for constructing written formative feedback on programming exercise submissions.},
  isbn = {978-1-4503-9616-5},
  keywords = {feedback,large language models,machine learning,natural language processing,program repair}
}

@inproceedings{kristiansen:2024:FeedbackStudentProgramming,
  title = {Feedback on {{Student Programming Assignments}}: {{Teaching Assistants}} vs {{Automated Assessment Tool}}},
  shorttitle = {Feedback on {{Student Programming Assignments}}},
  booktitle = {Proceedings of the 23rd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Kristiansen, Nynne Grauslund and Nicolajsen, Sebastian Mateos and Brabrand, Claus},
  year = {2024},
  month = feb,
  series = {Koli {{Calling}} '23},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3631802.3631804},
  urldate = {2024-06-13},
  abstract = {Existing research does not quantify and compare the differences between automated and manual assessment in the context of feedback on programming assignments. This makes it hard to reason about the effects of adopting automated assessment at the expense of manual assessment. Based on a controlled experiment involving N=117 undergraduate first-semester CS1 students, we compare the effects of having access to feedback from: i) only automated assessment, ii) only manual assessment (in the form of teaching assistants), and iii) both automated as well as manual assessment. The three conditions are compared in terms of (objective) task effectiveness and from a (subjective) student perspective. The experiment demonstrates that having access to both forms of assessment (automated and manual) is superior both from a task effectiveness as well as a student perspective. We also find that the two forms of assessment are complementary: automated assessment appears to be better in terms of task effectiveness; whereas manual assessment appears to be better from a student perspective. Further, we found that automated assessment appears to be working better for men than women, who are significantly more inclined towards manual assessment. We then perform a cost/benefit analysis which leads to the identification of four equilibria that appropriately balance costs and benefits. Finally, this gives rise to four recommendations of when to use which kind or combination of feedback (manual and/or automated), depending on the number of students and the amount of per-student resources available. These observations provide educators with evidence-based justification for budget requests and considerations on when to (not) use automated assessment.},
  isbn = {9798400716539},
  keywords = {automated assessment,feedback,student experiments,teaching assistants}
}

@inproceedings{krusche:2017:InteractiveLearningIncreasing,
  title = {Interactive {{Learning}}: {{Increasing Student Participation}} through {{Shorter Exercise Cycles}}},
  shorttitle = {Interactive {{Learning}}},
  booktitle = {Proceedings of the {{Nineteenth Australasian Computing Education Conference}}},
  author = {Krusche, Stephan and Seitz, Andreas and B{\"o}rstler, J{\"u}rgen and Bruegge, Bernd},
  year = {2017},
  month = jan,
  series = {{{ACE}} '17},
  pages = {17--26},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3013499.3013513},
  urldate = {2024-06-26},
  abstract = {In large classes, there is typically a clear separation between content delivery in lectures on the one hand and content deepening in practical exercises on the other hand. This temporal and spatial separation has several disadvantages. In particular, it separates students' hearing about a new concept from being able to actually practice and apply it, which may decrease knowledge retention.To closely integrate lectures and practical exercises, we propose an approach which we call interactive learning: it is based on active, computer based and experiential learning, includes immediate feedback and learning from the reflection on experience. It decreases the time between content delivery and content deepening to a few minutes and allows for flexible and more efficient learning. Shorter exercise cycles allow students to apply and practice multiple concepts per teaching unit directly after they first heard about them.We applied interactive learning in two large software engineering classes with 300 students each and evaluated its use qualitatively and quantitatively. The students' participation increases compared to traditional classes: until the end of the course, around 50\% of the students attend class and participate in exercises. Our evaluations show that students' learning experience and exam grades correlate with the increased participation. While educators need more time to prepare the class and the exercises, they need less time to review exercise submissions. The overall teaching effort for instructors and teaching assistants does not increase.},
  isbn = {978-1-4503-4823-2}
}

@inproceedings{krusche:2018:ArTEMiSAutomaticAssessmentf,
  title = {<placeholder due to the anonymized review>},
  booktitle = {},
  author = {Anonymized Author},
  year = {2018},
  pages = {},
}

@article{latif:2023:FinetuningChatGPTAutomatic,
  title = {Fine-Tuning {{ChatGPT}} for {{Automatic Scoring}}},
  author = {Latif, Ehsan and Zhai, Xiaoming},
  year = {2023},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2310.10072},
  urldate = {2024-03-26},
  abstract = {This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for automatically scoring student written constructed responses using example assessment tasks in science education. Recent studies on OpenAI's generative model GPT-3.5 proved its superiority in predicting the natural language with high accuracy and human-like responses. GPT-3.5 has been trained over enormous online language materials such as journals and Wikipedia; therefore, more than direct usage of pre-trained GPT-3.5 is required for automatic scoring as students utilize a different language than trained material. These imply that a domain-specific model, fine-tuned over data for specific tasks, can enhance model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks with a diverse dataset of middle-school and high-school student responses and expert scoring. The six tasks comprise two multi-label and four multi-class assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the fine-tuned state-of-the-art Google's generated language model, BERT. The results show that in-domain training corpora constructed from science questions and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5 shows a remarkable average increase (9.1\%) in automatic scoring accuracy (mean = 9.15, SD = 0.042) for the six tasks, p =0.001 \&lt; 0.05. Specifically, for multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5 achieved significantly higher scoring accuracy than BERT across all the labels, with the second item achieving a 7.1\% increase. The average scoring increase for the four multi-class items for GPT-3.5 was 10.6\% compared to BERT. Our study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring of student responses on domain-specific data in education with high accuracy. We have released fine-tuned models for public use and community engagement.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@article{lee:2023:ApplyingLargeLanguage,
  title = {Applying {{Large Language Models}} and {{Chain-of-Thought}} for {{Automatic Scoring}}},
  author = {Lee, Gyeong-Geon and Latif, Ehsan and Wu, Xuansheng and Liu, Ninghao and Zhai, Xiaoming},
  year = {2023},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2312.03748},
  urldate = {2024-03-26},
  abstract = {This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6\% increase. CoT, when used without item stem and scoring rubrics, did not significantly affect scoring accuracy (acc = .60). However, CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44\% increase for zero-shot; 3.7\% increase for few-shot). We found a more balanced accuracy across different proficiency categories when CoT was used with a scoring rubric, highlighting the importance of domain-specific reasoning in enhancing the effectiveness of LLMs in scoring tasks. We also found that GPT-4 demonstrated superior performance over GPT -3.5 in various scoring tasks when combined with the single-call greedy sampling or ensemble voting nucleus sampling strategy, showing 8.64\% difference. Particularly, the single-call greedy sampling strategy with GPT-4 outperformed other approaches.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@inproceedings{leinonen:2022:ComparisonImmediateScheduled,
  title = {A {{Comparison}} of {{Immediate}} and {{Scheduled Feedback}} in {{Introductory Programming Projects}}},
  booktitle = {Proceedings of the 53rd {{ACM Technical Symposium}} on {{Computer Science Education}} - {{Volume}} 1},
  author = {Leinonen, Juho and Denny, Paul and Whalley, Jacqueline},
  year = {2022},
  month = feb,
  series = {{{SIGCSE}} 2022},
  volume = {1},
  pages = {885--891},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3478431.3499372},
  urldate = {2024-06-25},
  abstract = {How students are assessed has a powerful effect on their strategies for studying and their learning. When designing assessments, instructors should consider how different approaches for providing feedback to students could encourage positive learning behaviours. One such design is the use of interim deadlines that enable students to receive and respond to feedback. This is used to encourage students to start early and thus reduce the negative effects of procrastination. If multiple submissions are allowed, penalty schemes can be included to encourage students to reflect deeply on the feedback they receive, rather than developing an over-reliance on autograders. In this work we describe two approaches to feedback used over two consecutive semesters for a final project in a large introductory programming course. In both semesters, the complexity and structure of the final project was similar and students received identical instruction. In the first instance of the course students could submit their work prior to two scheduled interim deadlines, after which they would receive automated feedback, before meeting a final third deadline. In the second instance, students received automated feedback immediately upon submission but with increasing penalties to discourage excessive submissions. In both cases, the ability to receive automated feedback -- both scheduled and immediate -- was designed to encourage early participation with the project. Under the two feedback schemes, we observed different patterns of behaviour -- particularly for the lower performing students. We explore the benefits and drawbacks of the two schemes and consider implications for future project grading.},
  isbn = {978-1-4503-9070-5}
}

@inproceedings{leinonen:2023:UsingLargeLanguage,
  title = {Using {{Large Language Models}} to {{Enhance Programming Error Messages}}},
  booktitle = {Proceedings of the 54th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Leinonen, Juho and Hellas, Arto and Sarsa, Sami and Reeves, Brent and Denny, Paul and Prather, James and Becker, Brett A.},
  year = {2023},
  month = mar,
  series = {{{SIGCSE}} 2023},
  pages = {563--569},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3545945.3569770},
  urldate = {2024-06-18},
  abstract = {A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix them. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.},
  isbn = {978-1-4503-9431-4},
  keywords = {ai,codex,compiler error messages,large language models,programming error messages,syntax error messages}
}

@inproceedings{liffiton:2024:CodeHelpUsingLarge,
  title = {{{CodeHelp}}: {{Using Large Language Models}} with {{Guardrails}} for {{Scalable Support}} in {{Programming Classes}}},
  shorttitle = {{{CodeHelp}}},
  booktitle = {Proceedings of the 23rd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Liffiton, Mark and Sheese, Brad E and Savelka, Jaromir and Denny, Paul},
  year = {2024},
  month = feb,
  series = {Koli {{Calling}} '23},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3631802.3631830},
  urldate = {2024-06-13},
  abstract = {Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students' usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.},
  isbn = {9798400716539},
  keywords = {Guardrails,Intelligent programming tutors,Intelligent tutoring systems,Large language models,Natural language interfaces,Novice programmers,Programming assistance}
}

@misc{liu:2024:JailbreakingChatGPTPrompt,
  title = {Jailbreaking {{ChatGPT}} via {{Prompt Engineering}}: {{An Empirical Study}}},
  shorttitle = {Jailbreaking {{ChatGPT}} via {{Prompt Engineering}}},
  author = {Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang},
  year = {2024},
  month = mar,
  number = {arXiv:2305.13860},
  eprint = {2305.13860},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13860},
  urldate = {2024-06-28},
  abstract = {Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering}
}

@inproceedings{liu:2024:TeachingAIGPT,
  title = {Teaching with {{AI}} ({{GPT}})},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 2},
  author = {Liu, Rongxin and Zenke, Carter and Lloyd, Doug and Malan, David J.},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {1902},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626253.3633433},
  urldate = {2024-06-15},
  abstract = {Teaching computer science at scale can be challenging. From our experience in CS50, Harvard University's introductory course, we've seen firsthand the impactful role that generative artificial intelligence can play in education. Recognizing its potential and stakes, we integrated OpenAI's GPT into our own teaching methodology. The goal was to emulate a 1:1 teacher-to-student ratio, incorporating "pedagogical guardrails" to maintain instructional integrity. The result was a personalized, AI-powered bot in the form of a friendly rubber duck aimed at delivering instructional responses and troubleshooting without giving outright solutions. We plan to share our journey and offer insights into responsibly harnessing AI in educational settings. Participants will gain hands-on experience working with GPT through OpenAI's APIs, understanding and crafting prompts, answering questions using embedding-based search, and finally, building their own AI chatbot. Ultimately, we'll not only share lessons learned from our own approach but also equip educators hands-on with the knowledge and tools with which they, too, can implement these technologies in their unique teaching environments.},
  isbn = {9798400704246},
  keywords = {ai,artificial intelligence,chatgpt,ethics,generative ai,gpt,programming,prompt,prompt engineering}
}

@inproceedings{liu:2024:TeachingCS50AIb,
  title = {Teaching {{CS50}} with {{AI}}: {{Leveraging Generative Artificial Intelligence}} in {{Computer Science Education}}},
  shorttitle = {Teaching {{CS50}} with {{AI}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Liu, Rongxin and Zenke, Carter and Liu, Charlie and Holmes, Andrew and Thornton, Patrick and Malan, David J.},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {750--756},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626252.3630938},
  urldate = {2024-06-15},
  abstract = {In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had "a personal tutor.'' Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education.},
  isbn = {9798400704239},
  keywords = {ai,artificial intelligence,generative ai,large language models,llms}
}

@inproceedings{liu:2024:TraditionalTeachingLargea,
  title = {Beyond {{Traditional Teaching}}: {{Large Language Models}} as {{Simulated Teaching Assistants}} in {{Computer Science}}},
  shorttitle = {Beyond {{Traditional Teaching}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Liu, Mengqi and M'Hiri, Faten},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {743--749},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626252.3630789},
  urldate = {2024-06-15},
  abstract = {As the prominence of Large Language Models (LLMs) grows in various sectors, their potential in education warrants exploration. In this study, we investigate the feasibility of employing GPT-3.5 from OpenAI, as an LLM teaching assistant (TA) or a virtual TA in computer science (CS) courses. The objective is to enhance the accessibility of CS education while maintaining academic integrity by refraining from providing direct solutions to current-semester assignments. Targeting Foundations of Programming (COMP202), an undergraduate course that introduces students to programming with Python, we have developed a virtual TA using the LangChain framework, known for integrating language models with diverse data sources and environments. The virtual TA assists students with their code and clarifies complex concepts. For homework questions, it is designed to guide students with hints rather than giving out direct solutions. We assessed its performance first through a qualitative evaluation, then a survey-based comparative analysis, using a mix of questions commonly asked on the COMP202 discussion board and questions created by the authors. Our preliminary results indicate that the virtual TA outperforms human TAs on clarity and engagement, matching them on accuracy when the question is non-assignment-specific, for which human TAs still proved more reliable. These findings suggest that while virtual TAs, leveraging the capabilities of LLMs, hold great promise towards making CS education experience more accessible and engaging, their optimal use necessitates human supervision. We conclude by identifying several directions that could be explored in future implementations.},
  isbn = {9798400704239},
  keywords = {adaptive teaching,chatgpt,cs education,gpt,llm,machine learning,novice programmers,openai,programming}
}

@inproceedings{malaise:2024:ExplorotronIDEExtension,
  title = {Explorotron: {{An IDE Extension}} for {{Guided}} and {{Independent Code Exploration}} and {{Learning}} ({{Discussion Paper}})},
  shorttitle = {Explorotron},
  booktitle = {Proceedings of the 23rd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Malaise, Yoshi and Signer, Beat},
  year = {2024},
  month = feb,
  series = {Koli {{Calling}} '23},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3631802.3631816},
  urldate = {2024-06-14},
  abstract = {We introduce the Explorotron Visual Studio Code extension for guided and independent code exploration and learning. Explorotron is a continuation of earlier work to explore how we can enable small organisations with limited resources to provide pedagogically sound learning experiences in programming. We situate Explorotron in the field of Computing Education Research (CER) and envision it to initiate a discussion around different topics, including how to balance the optimisation between the researcher-student-teacher trifecta that is inherent in CER, how to ethically and responsibly use large language models (LLMs) in the independent learning and exploration by students, and how to define better learning sessions over coding content that students obtained on their own. We further reflect on the question raised by Begel and Ko whether technology should ``structure learning for learners'' or whether learners should ``be taught how to structure their own independent learning'' outside of the classroom.},
  isbn = {9798400716539},
  keywords = {PRIMM,Programming Education,Study Lenses}
}

@inproceedings{malysheva:2022:AlgorithmGeneratingExplainable,
  title = {An {{Algorithm}} for {{Generating Explainable Corrections}} to {{Student Code}}},
  booktitle = {Proceedings of the 22nd {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Malysheva, Yana and Kelleher, Caitlin},
  year = {2022},
  month = nov,
  series = {Koli {{Calling}} '22},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3564721.3564731},
  urldate = {2024-06-14},
  abstract = {Students in introductory computer science courses often need individualized help when they get stuck solving programming problems. But providing such help can be time-consuming and thought-intensive, and therefore difficult to scale as Computer Science classes grow larger in size. Automatically generated fixes with explanations have the potential to integrate into a variety of mechanisms for providing help to students who are stuck on a programming problem. In this paper, we present a data-driven algorithm for generating explainable fixes to student code. We evaluate a Python implementation of the algorithm by comparing its output at different stages of the algorithm to state-of-the-art systems with similar goals. Our algorithm outperforms existing systems that can analyze and fix beginner-written Python code. Further, fixes it generates conform very well to corrections written by human experts for an existing benchmark of code correction quality.},
  isbn = {978-1-4503-9616-5}
}

@inproceedings{marwan:2020:AdaptiveImmediateFeedback,
  title = {Adaptive {{Immediate Feedback Can Improve Novice Programming Engagement}} and {{Intention}} to {{Persist}} in {{Computer Science}}},
  booktitle = {Proceedings of the 2020 {{ACM Conference}} on {{International Computing Education Research}}},
  author = {Marwan, Samiha and Gao, Ge and Fisk, Susan and Price, Thomas W. and Barnes, Tiffany},
  year = {2020},
  month = aug,
  series = {{{ICER}} '20},
  pages = {194--203},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3372782.3406264},
  urldate = {2024-06-25},
  abstract = {Prior work suggests that novice programmers are greatly impacted by the feedback provided by their programming environments. While some research has examined the impact of feedback on student learning in programming, there is no work (to our knowledge) that examines the impact of adaptive immediate feedback within programming environments on students' desire to persist in computer science (CS). In this paper, we integrate an adaptive immediate feedback (AIF) system into a block-based programming environment. Our AIF system is novel because it provides personalized positive and corrective feedback to students in real time as they work. In a controlled pilot study with novice high-school programmers, we show that our AIF system significantly increased students' intentions to persist in CS, and that students using AIF had greater engagement (as measured by their lower idle time) compared to students in the control condition. Further, we found evidence that the AIF system may improve student learning, as measured by student performance in a subsequent task without AIF. In interviews, students found the system fun and helpful, and reported feeling more focused and engaged. We hope this paper spurs more research on adaptive immediate feedback and the impact of programming environments on students' intentions to persist in CS.},
  isbn = {978-1-4503-7092-9}
}

@misc{mosbach:2023:FewshotFinetuningVs,
  title = {Few-Shot {{Fine-tuning}} vs. {{In-context Learning}}: {{A Fair Comparison}} and {{Evaluation}}},
  shorttitle = {Few-Shot {{Fine-tuning}} vs. {{In-context Learning}}},
  author = {Mosbach, Marius and Pimentel, Tiago and Ravfogel, Shauli and Klakow, Dietrich and Elazar, Yanai},
  year = {2023},
  month = may,
  number = {arXiv:2305.16938},
  eprint = {2305.16938},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16938},
  urldate = {2024-06-28},
  abstract = {Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{nguyen:2024:UsingGPT4Providea,
  title = {Using {{GPT-4}} to {{Provide Tiered}}, {{Formative Code Feedback}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Nguyen, Ha and Allan, Vicki},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {958--964},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626252.3630960},
  urldate = {2024-06-15},
  abstract = {Large language models (LLMs) have shown promise in generating sensible code explanation and feedback in programming exercises. In this experience report, we discuss the process of using one of these models (OpenAI's GPT-4) to generate individualized feedback for students' Java code and pseudocode. We instructed GPT-4 to generate feedback for 113 submissions to four programming problems in an Algorithms and Data Structures class. We prompted the model with example feedback (few-shot learning) and instruction to (1) give feedback on conceptual understanding, syntax, and time complexity, and (2) suggest follow-up actions based on students' code or provide guiding questions. Overall, GPT-4 provided accurate feedback and successfully built on students' ideas in most submissions. Human evaluators (computer science instructors and tutors) rated GPT-4's hints as useful in guiding students' next steps. Model performance varied with programming problems but not submission quality. We reflect on where the model performed well and fell short, and discuss the potential of integrating LLM-generated, individualized feedback into computer science instruction.},
  isbn = {9798400704239},
  keywords = {computer science education,feedback,large language models}
}

@article{paiva:2022:AutomatedAssessmentComputer,
  title = {Automated {{Assessment}} in {{Computer Science Education}}: {{A State-of-the-Art Review}}},
  shorttitle = {Automated {{Assessment}} in {{Computer Science Education}}},
  author = {Paiva, Jos{\'e} Carlos and Leal, Jos{\'e} Paulo and Figueira, {\'A}lvaro},
  year = {2022},
  month = sep,
  journal = {ACM Transactions on Computing Education},
  volume = {22},
  number = {3},
  pages = {1--40},
  issn = {1946-6226, 1946-6226},
  doi = {10.1145/3513140},
  urldate = {2024-03-26},
  abstract = {Practical programming competencies are critical to the success in computer science (CS) education and go-to-market of fresh graduates. Acquiring the required level of skills is a long journey of discovery, trial and error, and optimization seeking through a broad range of programming activities that learners must perform themselves. It is not reasonable to consider that teachers could evaluate all attempts that the average learner should develop multiplied by the number of students enrolled in a course, much less in a timely, deep, and fair fashion. Unsurprisingly, exploring the formal structure of programs to automate the assessment of certain features has long been a hot topic among CS education practitioners. Assessing a program is considerably more complex than asserting its functional correctness, as the proliferation of tools and techniques in the literature over the past decades indicates. Program efficiency, behavior, and readability, among many other features, assessed either statically or dynamically, are now also relevant for automatic evaluation. The outcome of an evaluation evolved from the primordial Boolean values to information about errors and tips on how to advance, possibly taking into account similar solutions. This work surveys the state of the art in the automated assessment of CS assignments, focusing on the supported types of exercises, security measures adopted, testing techniques used, type of feedback produced, and the information they offer the teacher to understand and optimize learning. A new era of automated assessment, capitalizing on static analysis techniques and containerization, has been identified. Furthermore, this review presents several other findings from the conducted review, discusses the current challenges of the field, and proposes some future research directions.},
  langid = {english}
}

@inproceedings{park:2023:EliRankCodeEditing,
  title = {{{EliRank}}: {{A Code Editing History Based Ranking Model}} for {{Early Detection}} of {{Students}} in {{Need}}},
  shorttitle = {{{EliRank}}},
  booktitle = {Proceedings of the {{Tenth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Park, Jungkook and Oh, Alice},
  year = {2023},
  month = jul,
  series = {L@{{S}} '23},
  pages = {204--214},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3573051.3593387},
  urldate = {2024-06-14},
  abstract = {Research on programming education shows that novice programming students benefit significantly from one-to-one tutoring. While many systems propose to replicate the effectiveness of one-to-one tutoring in large-scale classes, it remains a challenge to develop systems with an approach to finding students who need the tutors' help the most. In this paper, we explore the idea of predicting the priority of students in need with a data-driven approach. Among various metrics to calculate the priority of students in need, we adopt time-on-task metric. Previous studies have found that excessively long time-on-task can be used as an indication of students' struggling. Aligned with this, we reduce the problem of finding students with the highest priority to the problem of finding students with the longest time-on-task. To solve the reduced problem, we present EliRank, a ranking model that finds students with the longest estimated time-on-task, using the students' first few minutes of fine-grained code editing history. EliRank recommends students in the descending order of estimated time-on-task, enabling tutors to efficiently monitor and find the students in need at scale in real time. To evaluate the performance of EliRank, we build and publish a new real-world dataset consisting of 15 programming exercises solved by 4000+ students in an introduction to programming class at a university. Unlike the currently available open code editing history datasets, our dataset contains code editing operations at a character-level granularity to minimize the loss of contextual information from students. We also introduce diff-augmented abstract syntax tree (DAST), a novel structured code representation that minimizes the loss of fine-grained code change information during code parsing. The evaluation of EliRank on our dataset shows that EliRank effectively finds students with the longest estimated time-on-task, for early detection of students in need. Also, we illustrate in depth (i) the effectiveness of DAST, (ii) the potential to control the tradeoff between early detection and the prediction accuracy of the model, and (iii) the transferability to unseen programming exercise via zero-shot transfer learning.},
  isbn = {9798400700255},
  keywords = {code editing history,machine learning,programming education,recommendation system}
}

@inproceedings{prihar:2022:AutomaticInterpretablePersonalized,
  title = {Automatic {{Interpretable Personalized Learning}}},
  booktitle = {Proceedings of the {{Ninth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Prihar, Ethan and Haim, Aaron and Sales, Adam and Heffernan, Neil},
  year = {2022},
  month = jun,
  series = {L@{{S}} '22},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3491140.3528267},
  urldate = {2024-06-14},
  abstract = {Personalized learning stems from the idea that students benefit from instructional material tailored to their needs. Many online learning platforms purport to implement some form of personalized learning, often through on-demand tutoring or self-paced instruction, but to our knowledge none have a way to automatically explore for specific opportunities to personalize students' education nor a transparent way to identify the effects of personalization on specific groups of students. In this work we present the Automatic Personalized Learning Service (APLS). The APLS uses multi-armed bandit algorithms to recommend the most effective support to each student that requests assistance when completing their online work, and is currently used by ASSISTments, an online learning platform. The first empirical study of the APLS found that Beta-Bernoulli Thompson Sampling, a popular and effective multi-armed bandit algorithm, was only slightly more capable of selecting helpful support than randomly selecting from the relevant support options. Therefore, we also present Decision Tree Thompson Sampling (DTTS), a novel contextual multi-armed bandit algorithm that integrates the transparency and interpretability of decision trees into Thomson sampling. In simulation, DTTS overcame the challenges of recommending support within an online learning platform and was able to increase students' learning by as much as 10\% more than the current algorithm used by the APLS. We demonstrate that DTTS is able to identify qualitative interactions that not only help determine the most effective support for students, but that also generalize well to new students, problems, and support content. The APLS using DTTS is now being deployed at scale within ASSISTments and is a promising tool for all educational learning platforms.},
  isbn = {978-1-4503-9158-0},
  keywords = {contextual bandits,intelligent tutoring systems,multi-armed bandits,personalized learning}
}

@misc{qiao:2023:ReasoningLanguageModel,
  title = {Reasoning with {{Language Model Prompting}}: {{A Survey}}},
  shorttitle = {Reasoning with {{Language Model Prompting}}},
  author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  year = {2023},
  month = sep,
  number = {arXiv:2212.09597},
  eprint = {2212.09597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09597},
  urldate = {2024-06-28},
  abstract = {Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@inproceedings{risha:2021:StepwiseHelpScaffolding,
  title = {Stepwise {{Help}} and {{Scaffolding}} for {{Java Code Tracing Problems With}} an {{Interactive Trace Table}}},
  booktitle = {Proceedings of the 21st {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Risha, Zak and {Barria-Pineda}, Jordan and Akhuseyinoglu, Kamil and Brusilovsky, Peter},
  year = {2021},
  month = nov,
  series = {Koli {{Calling}} '21},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3488042.3490508},
  urldate = {2024-06-14},
  abstract = {In this paper, we describe the integration of a step-by-step interactive trace table into an existing practice system for introductory Java programming. These autogenerated trace problems provide help and scaffolding for students who have trouble in solving traditional one-step code tracing problems, accommodating a wider variety of learners. Findings from classroom deployments suggest the scaffolding provided by the trace table is a plausible form of help, most notably increases in performance and persistence and lower task difficulty. Based on usage data, we propose future implications for an adaptive version of the interactive trace table based on learner modeling.},
  isbn = {978-1-4503-8488-9},
  keywords = {computer science education,educational research,educational tools,help design,intelligent tutoring systems,smart learning content,teaching with technology,technology integration,tracing}
}

@inproceedings{roest:2024:NextStepHintGeneration,
  title = {Next-{{Step Hint Generation}} for {{Introductory Programming Using Large Language Models}}},
  booktitle = {Proceedings of the 26th {{Australasian Computing Education Conference}}},
  author = {Roest, Lianne and Keuning, Hieke and Jeuring, Johan},
  year = {2024},
  month = jan,
  series = {{{ACE}} '24},
  pages = {144--153},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3636243.3636259},
  urldate = {2024-06-18},
  abstract = {Large Language Models possess skills such as answering questions, writing essays or solving programming exercises. Since these models are easily accessible, researchers have investigated their capabilities and risks for programming education. This work explores how LLMs can contribute to programming education by supporting students with automated next-step hints. We investigate prompt practices that lead to effective next-step hints and use these insights to build our StAP-tutor. We evaluate this tutor by conducting an experiment with students, and performing expert assessments. Our findings show that most LLM-generated feedback messages describe one specific next step and are personalised to the student's code and approach. However, the hints may contain misleading information and lack sufficient detail when students approach the end of the assignment. This work demonstrates the potential for LLM-generated feedback, but further research is required to explore its practical implementation.},
  isbn = {9798400716195},
  keywords = {automated feedback,Generative AI,Large Language Models,learning programming,Next-step hints}
}

@article{runeson:2009:GuidelinesConductingReporting,
  title = {Guidelines for Conducting and Reporting Case Study Research in Software Engineering},
  author = {Runeson, Per and H{\"o}st, Martin},
  year = {2009},
  month = apr,
  journal = {Empirical Software Engineering},
  volume = {14},
  number = {2},
  pages = {131--164},
  issn = {1573-7616},
  doi = {10.1007/s10664-008-9102-8},
  abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.}
}

@article{schartel:2012:GivingFeedbackIntegral,
  title = {Giving Feedback -- {{An}} Integral Part of Education},
  author = {Schartel, Scott A.},
  year = {2012},
  month = mar,
  journal = {Best Practice \& Research Clinical Anaesthesiology},
  series = {Challenges in {{Anaesthesia Education}}},
  volume = {26},
  number = {1},
  pages = {77--87},
  issn = {1521-6896},
  doi = {10.1016/j.bpa.2012.02.003},
  urldate = {2024-06-13},
  abstract = {Feedback is an integral part of the educational process. It provides learners with a comparison of their performance to educational goals with the aim of helping them achieve or exceed their goals. Effective feedback is delivered in an appropriate setting, focusses on performance and not the individual, is specific, is based on direct observation or objective date, is delivered using neutral, non-judgemental language and identifies actions or plans for improvement. For best results, the sender and receiver of feedback must work as allies. Negative feedback can create an emotional response in the learner, which may interfere with the effectiveness of the feedback due to dissonance between self-evaluation and external appraisal. Reflection can help learners process negative feedback and allow them to develop and implement improvement plans. Both delivering and receiving feedback are skills that can be improved with training. Teachers have a duty to provide meaningful feedback to learners; learners should expect feedback and seek it.},
  keywords = {clinical competence,education medical/standards,feedback,knowledge of results (psychology),learning,self-assessment,self-concept,teaching/methods,teaching/trends}
}

@article{shute:2008:FocusFormativeFeedback,
  title = {Focus on {{Formative Feedback}}},
  author = {Shute, Valerie J.},
  year = {2008},
  month = mar,
  journal = {Review of Educational Research},
  volume = {78},
  number = {1},
  pages = {153--189},
  publisher = {American Educational Research Association},
  issn = {0034-6543},
  doi = {10.3102/0034654307313795},
  urldate = {2024-06-25},
  abstract = {This article reviews the corpus of research on feedback, with a focus on formative feedback---defined as information communicated to the learner that is intended to modify his or her thinking or behavior to improve learning. According to researchers, formative feedback should be nonevaluative, supportive, timely, and specific. Formative feedback is usually presented as information to a learner in response to some action on the learner's part. It comes in a variety of types (e.g., verification of response accuracy, explanation of the correct answer, hints, worked examples) and can be administered at various times during the learning process (e.g., immediately following an answer, after some time has elapsed). Finally, several variables have been shown to interact with formative feedback's success at promoting learning (e.g., individual characteristics of the learner and aspects of the task). All of these issues are discussed. This review concludes with guidelines for generating formative feedback.},
  langid = {english}
}

@inproceedings{sondergaard:2004:EffectiveFeedbackSmall,
  title = {Effective Feedback to Small and Large Classes},
  booktitle = {34th {{Annual Frontiers}} in {{Education}}, 2004. {{FIE}} 2004.},
  author = {Sondergaard, H. and Thomas, D.},
  year = {2004},
  pages = {540--545},
  publisher = {IEEE},
  address = {Savannah, GA, USA},
  doi = {10.1109/FIE.2004.1408573},
  urldate = {2024-06-26},
  abstract = {Educational experts appear to be in broad agreement when it comes to the importance of feedback for effective learning. Students benefit from plenty of opportunity and encouragement to express their understanding, and from informed, supportive, possibly challenging, feedback. At the same time, we observe that many students at our university do not find that they receive helpful feedback. One in three engineering students disagree or strongly disagree with the Quality of Teaching questionnaire's "I received helpful feedback on how I was going" in the individual course, and most other disciplines find themselves in a similar situation. For the university as a whole, student responses to this question are clearly less positive than to other questions on quality of teaching, intellectual stimulation, staff interest, workload, and so on, and this state of affairs seems quite common in the Australian context. We discuss best practice in feedback provision, partly based on our interviews with students and staff. We have been particularly interested in identifying cost-effective ways of providing informed and constructive feedback to large classes. Feedback is often understood, by engineering students and staff alike, simply as comments on submitted work - typically written assignments. We argue in favour of a broader concept that covers a multitude of ways for a student to develop deep learning through conversation, including questions and answers provided by others, team work, study groups, and formative teacher-provided feedback during an assessment task. We emphasise the coaching role of the teacher, and feedback designed to encourage students to monitor own learning. Large classes pose particular logistic problems. We identify staff development as a crucial factor for consistent, effective feedback, and point to web-based feedback provision as a workable solution to some logistic problems. We briefly discuss the role of information technology more broadly, both for learning enhancement and for automated feedback provision.},
  isbn = {978-0-7803-8552-8},
  langid = {english}
}

@misc{taylor:2023:DccHelpGenerating,
  title = {Dcc --Help: {{Generating Context-Aware Compiler Error Explanations}} with {{Large Language Models}}},
  shorttitle = {Dcc --Help},
  author = {Taylor, Andrew and Vassar, Alexandra and Renzella, Jake and Pearce, Hammond},
  year = {2023},
  month = oct,
  number = {arXiv:2308.11873},
  eprint = {2308.11873},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.11873},
  urldate = {2024-06-15},
  abstract = {In the challenging field of introductory programming, high enrollments and failure rates drive us to explore tools and systems to enhance student outcomes, especially automated tools that scale to large cohorts. This paper presents and evaluates the dcc --help tool, an integration of a Large Language Model (LLM) into the Debugging C Compiler (DCC) to generate unique, novice-focused explanations tailored to each error. dcc --help prompts an LLM with contextual information of compile- and run-time error occurrences, including the source code, error location and standard compiler error message. The LLM is instructed to generate novice-focused, actionable error explanations and guidance, designed to help students understand and resolve problems without providing solutions. dcc --help was deployed to our CS1 and CS2 courses, with 2,565 students using the tool over 64,000 times in ten weeks. We analysed a subset of these error/explanation pairs to evaluate their properties, including conceptual correctness, relevancy, and overall quality. We found that the LLM-generated explanations were conceptually accurate in 90\% of compile-time and 75\% of run-time cases, but often disregarded the instruction not to provide solutions in code. Our findings, observations and reflections following deployment indicate that dcc-help provides novel opportunities for scaffolding students' introduction to programming.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering}
}

@inproceedings{vandenaker:2024:DesignPrinciplesGeneratingb,
  title = {Design Principles for Generating and Presenting Automated Formative Feedback on Code Quality Using Software Metrics},
  booktitle = {Proceedings of the 46th {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}}},
  author = {{van den Aker}, Eddy and Rahimi, Ebrahim},
  year = {2024},
  month = may,
  series = {{{ICSE-SEET}} '24},
  pages = {139--150},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3639474.3640051},
  urldate = {2024-06-24},
  abstract = {Code quality and maintainability are among under-emphasized and often neglected topics in the curriculum of software engineering (SE) in higher education. This neglect tends to overlook research findings that demonstrate SE students' programming submissions most often exhibit severe code quality issues, which are frequently left unaddressed by the students. Furthermore, it can result in the software engineering curriculum becoming indifferent to the essential requirements of the software development industry, where code quality and maintainability play a crucial role in the software's cost throughout its life cycle.Therefore, SE students in higher education should be trained to master the knowledge and skills of writing high-quality code. One possible approach to improving students' understanding of code quality issues is to provide automatically generated formative feedback about the code quality aspects of their programming submissions throughout the code development process. However, while there are tools available for generating automated feedback on the code quality aspects of programming submissions, they often lack a set of theory-driven design principles to underpin the content and presentation of their provided feedback. This lack of theoretical foundation makes it difficult to follow a systematic approach to designing and developing such tools, reasoning about their quality, and evaluating the effectiveness of their generated feedback.To address this lack, this study provides nine contextualized design principles for generating automated formative feedback on code quality. These design principles are rooted in solid educational constructs about feedback and learning dashboards, and empirically validated and contextualized by two focus group sessions consisting of 8 senior SE students and 2 teachers.This approach has resulted in a set of contextualized design principles. These design principles can be used to guide the implementation of tools that provide automated feedback on code quality using software metrics.},
  isbn = {9798400704987}
}

@misc{wei:2022:EmergentAbilitiesLarge,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.07682},
  urldate = {2024-06-28},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{wei:2023:ChainofThoughtPromptingElicitsa,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2024-06-28},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@book{wieringa:2014:DesignScienceMethodologya,
  title = {Design {{Science Methodology}} for {{Information Systems}} and {{Software Engineering}}},
  author = {Wieringa, Roel J.},
  year = {2014},
  publisher = {Springer Berlin Heidelberg}
}

@inproceedings{woodrow:2024:AITeachesArt,
  title = {{{AI Teaches}} the {{Art}} of {{Elegant Coding}}: {{Timely}}, {{Fair}}, and {{Helpful Style Feedback}} in a {{Global Course}}},
  shorttitle = {{{AI Teaches}} the {{Art}} of {{Elegant Coding}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Woodrow, Juliette and Malik, Ali and Piech, Chris},
  year = {2024},
  month = mar,
  series = {{{SIGCSE}} 2024},
  pages = {1442--1448},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626252.3630773},
  urldate = {2024-06-18},
  abstract = {Teaching students how to write code that is elegant, reusable, and comprehensible is a fundamental part of CS1 education. However, providing this "style feedback" in a timely manner has proven difficult to scale. In this paper, we present our experience deploying a novel, real-time style feedback tool in Code in Place, a large-scale online CS1 course. Our tool is based on the latest breakthroughs in large-language models (LLMs) and was carefully designed to be safe and helpful for students. We used our Real-Time Style Feedback tool (RTSF) in a class with over 8,000 diverse students from across the globe and ran a randomized control trial to understand its benefits. We show that students who received style feedback in real-time were five times more likely to view and engage with their feedback compared to students who received delayed feedback. Moreover, those who viewed feedback were more likely to make significant style-related edits to their code, with over 79\% of these edits directly incorporating their feedback. We also discuss the practicality and dangers of LLM-based tools for feedback, investigating the quality of the feedback generated, LLM limitations, and techniques for consistency, standardization, and safeguarding against demographic bias, all of which are crucial for a tool utilized by students.},
  isbn = {9798400704239},
  keywords = {cs1,deployed at scale,gpt,llms,real time,style feedback}
}

@inproceedings{xue:2024:DoesChatGPTHelpa,
  title = {Does {{ChatGPT}} Help with Introductory {{Programming}}?{{An}} Experiment of Students Using {{ChatGPT}} in {{CS1}}},
  booktitle = {Proceedings of the 46th International Conference on Software Engineering: {{Software}} Engineering Education and Training},
  author = {Xue, Yuankai and Chen, Hanlin and Bai, Gina R. and Tairas, Robert and Huang, Yu},
  year = {2024},
  series = {{{ICSE-SEET}} '24},
  pages = {331--341},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3639474.3640076},
  abstract = {Generative AI, notably ChatGPT, has garnered attention in computer science education. This paper presents a controlled experiment that explores ChatGPT's role in CS1 in a classroom setting. Specifically, we aim to investigate the impact of ChatGPT on student learning outcomes and their behaviors when working on programming assignments. Participants were tasked with creating a UML diagram and subsequently implementing its design through programming, followed by a closed-book post-evaluation and a post-survey. All the participants were required to screen-record the whole process. In total, 56 participants were recruited, with 48 successful screen recordings. Participants in the Experimental Group can access ChatGPT 3.5 and other online resources, such as Google and Stack Overflow when creating the UML diagram and programming; however, participants in the Control Group can access all online resources except for ChatGPT (i.e., the only design variable is the access to ChatGPT). Finally, we measured and analyzed participants' learning outcomes through their UML diagram, programming, and post-evaluation scores. We also analyzed the time participants took to complete the tasks and their interactions with ChatGPT and other resources from the screen recordings. After finishing the tasks, student participants also provided their perceptions of using ChatGPT in CS1 through a post-survey.With rigorous quantitative and qualitative analysis, we found that (1) using ChatGPT does not present a significant impact on students' learning performance in the CS1 assignment-style tasks; (2) once using ChatGPT, students' tendency to explore other traditional educational resources is largely reduced (though available) and they tend to rely solely on ChatGPT, and this reliance on ChatGPT did not guarantee enhanced learning performance; (3) the majority of students hold neutral views on ChatGPT's role in CS1 programming but most of them raised concerns about its potential ethical issues and inconsistent performance across different tasks. We hope this study can help educators and students better understand the impact of ChatGPT in CS1 and inspire future work to provide proper guidelines for using ChatGPT in introductory programming classes.},
  isbn = {9798400704987},
  keywords = {ChatGPT,CS education,CS1,generative AI,OOP}
}

@article{yang:2024:HarnessingPowerLLMs,
  title = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}: {{A Survey}} on {{ChatGPT}} and {{Beyond}}},
  shorttitle = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}},
  author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  year = {2024},
  month = apr,
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {18},
  number = {6},
  pages = {160:1--160:32},
  issn = {1556-4681},
  doi = {10.1145/3649506},
  urldate = {2024-06-28},
  abstract = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at . An LLMs evolutionary tree, editable yet regularly updated, can be found at  .}
}

@misc{yao:2023:ReActSynergizingReasoning,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2024-06-28},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{zhang:2024:PyDexRepairingBugs,
  title = {{{PyDex}}: {{Repairing Bugs}} in {{Introductory Python Assignments}} Using {{LLMs}}},
  shorttitle = {{{PyDex}}},
  author = {Zhang, Jialu and Cambronero, Jos{\'e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust},
  year = {2024},
  month = apr,
  journal = {Proc. ACM Program. Lang.},
  volume = {8},
  number = {OOPSLA1},
  pages = {133:1100--133:1124},
  doi = {10.1145/3649850},
  urldate = {2024-06-25},
  abstract = {Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system -- PyDex -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.}
}

@misc{zhao:2023:SurveyLargeLanguage,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = nov,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.18223},
  urldate = {2024-06-28},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}
