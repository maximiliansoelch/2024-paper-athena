%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,screen,review]{acmart}

\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage[capitalize, noabbrev]{cleveref}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{enumitem}

\usepackage{pgfplots}
\usepackage{listings}

\lstdefinestyle{prompt}{
  basicstyle=\small,
  breaklines=true,
  breakindent=0pt,
  frame=single,
  escapeinside={\%*}{*)},  % if you want to add LaTeX within your code
}

\lstset{
  basicstyle=\small, % or \footnotesize, \scriptsize, etc.
  % other settings
}

\newcommand{\highlight}[1]{\textcolor{blue}{\textbf{#1}}}

% commands for the findings box
\definecolor{main}{HTML}{808080}    % setting main color to be used
\definecolor{sub}{HTML}{f2f2f2} 
\tcbset{
	sharp corners,
	colback = white,
	before skip = 0.2cm,    % add extra space before the box
	after skip = 0.5cm      % add extra space after the box
}
\newtcolorbox{findingsBox}{
	colback = sub, 
	colframe = main, 
	boxrule = 0pt, 
	leftrule = 6pt % left rule weight
}    


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Koli Calling '24]{24th Koli Calling International Conference on Computing Education Research}{November 14--17,
  2024}{Koli, Finland}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{Automated Feedback Loops: Enhancing Student Motivation and Performance in Programming Courses}
\title{Direct Automated Feedback Delivery for Student Submissions based on LLMs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Maximilian Sölch}
\email{maximilian.soelch@tum.de}
\orcid{0009-0004-1509-7842} 
\affiliation{%
	\institution{Technical University of Munich}
	\city{Munich}
	\country{Germany}
}

\author{Felix T.J. Dietrich}
\email{felixtj.dietrich@tum.de}
\orcid{0009-0007-5826-2061} 
\affiliation{%
	\institution{Technical University of Munich}
	\city{Munich}
	\country{Germany}
}

\author{Stephan Krusche}
\email{krusche@tum.de}
\orcid{0000-0002-4552-644X}
\affiliation{%
	\institution{Technical University of Munich}
	\city{Munich}
	\country{Germany}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Sölch et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

Timely and individualized feedback is crucial for students' learning progress and motivation.
However, providing such feedback is a major challenge in education, especially as the number of students has steadily increased in recent years.
This growth has made it difficult for teachers to provide individualized feedback to each student, resulting in a time-consuming, repetitive and often manual task that contributes to a high workload.

This paper presents Direct Automated Feedback (DAFeeD), a large language model (LLM)-based approach for automated formative feedback on student submissions in various exercise domains.
The defined feedback process enables interactive learning by allowing students to submit their solutions multiple times and request feedback repeatedly before the submission deadline.
By incorporating task-specific information into the prompt, DAFeeD provides iterative customized feedback, facilitating continuous student improvement.

To empirically evaluate the feedback process, we implemented it in an open-source reference implementation that is integrated into the LMS learning platform.
We conducted a controlled study in which students used the feedback process in a programming task in a supervised environment and completed a survey.
The results show that students perceive the feedback process as relevant and beneficial to their learning.
Students indicated that they feel more comfortable and willing to request automated feedback than they would with human tutors because it is more convenient and immediate.
This shows that DAFeeD has the potential to significantly improve the feedback process in educational institutions and increase students' learning efficiency and performance.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
  <ccs2012>
     <concept>
         <concept_id>10003456.10003457.10003527.10003540</concept_id>
         <concept_desc>Social and professional topics~Student assessment</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
     <concept>
         <concept_id>10010405.10010489</concept_id>
         <concept_desc>Applied computing~Education</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
   </ccs2012>
\end{CCSXML}
  
\ccsdesc[500]{Social and professional topics~Student assessment}
\ccsdesc[500]{Applied computing~Education}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Software Engineering, Education, Formative Feedback, Interactive Learning}

% \received{20 February 2007}
% \received[revised]{12 March 2009} 
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction} % 1 page

% problem
% time consuming, not scalable, not always available
% hinders learning progress and motivation %todo: reference for this!

% todo: reference for this!
In the current educational landscape, providing timely and effective feedback to students remains a significant challenge.
Traditionally, students must wait for course tutors or professors to review their submissions and provide feedback.
This process can be time-consuming, often requiring students to arrange meetings and wait for available time slots, which are not always convenient or immediate.
Similar it is timecosuming and tedious for professors and tutors to provide asynchronous feedback via email or other communication channels.
The inherent delays and scheduling difficulties make this approach not scalable, especially in courses with a large number of students.

%todo: interactive learning concept (ref stephans paper)
% todo: reference for this!
These limitations hinder students' learning progress and motivation.
The waiting period for feedback interrupts the learning flow, causing students to lose momentum and potentially disengage from the subject matter.
%todo: reference for this!
Additionally, the limited availability of tutors and professors means that not all students receive the individualized attention they need to improve their understanding and skills.
This situation underscores the necessity for a more efficient and scalable feedback system that can provide continuous support to students without the constraints of traditional methods.


% Objectives
In this paper, we present an approach for generating automated feedback on student submissions using large language models (LLMs) to address these challenges.
The approach is independent of the exercise type and can be applied to various domains, such as programming, text, or modeling exercises.
We implemented the approach in an open-source reference implementation called Athena, connected to the learning platform Artemis through which students submit their solutions and receive feedback.
To validate the effectiveness and efficiency of the approach, we tested it in a controlled environment. 
We collected quantitative and qualitative data to evaluate students' perceptions of the approach and the overall performance of the reference implementation.
The results show %todo: add results


%Outline
The subsequent sections of this paper are organized to provide a comprehensive understanding of the research. 
\cref{sec:related-work} provides an overview of related work. 
\cref{sec:approach:DAFeeD} details the concept and methodology of Direct Automated Feedback Delivery (DAFeeD).
\cref{sec:reference-implementation} describes the reference implementation of DAFeeD, called Athena, including a general overview, details on the used prompts, and the system architecture. 
\cref{sec:evaluation} presents the evaluation results, %todo: add results 
Finally, \cref{sec:conclusion} concludes with a summary of findings and discusses future research directions to enhance automated feedback systems.

%\newpage
\section{Related Work} % 2 page
\label{sec:related-work}

% > Effective Feedback Practices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Focus on Formative Feedback
% \citet{shute:2008:FocusFormativeFeedback} 

% What makes for effective feedback: staff and student perspectives
% \citet{dawson:2019:WhatMakesEffective}


% > Impact of Feedback on Student Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Adaptive Immediate Feedback Can Improve Novice Programming Engagement and Intention to Persist in Computer Science
% \citet{marwan:2020:AdaptiveImmediateFeedback}

% A Comparison of Immediate and Scheduled Feedback in Introductory Programming Projects
% \citet{leinonen:2022:ComparisonImmediateScheduled}


% > Automated Feedback Systems in Education %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% todo!: A review of automated feedback systems for learners: Classification framework, challenges, and opportunities.
% \citet{deeva:2021:ReviewAutomatedFeedback}

% A Systematic Review of the Effects of Automatic Scoring and Automatic Feedback in Educational Settings
% Marcelo Guerra Hahn, Silvia Margarita Baldiris Navarro, Luis De La Fuente Valentin, and Daniel Burgos. 2021. A Systematic Review of the Effects of Automatic Scoring and Automatic Feedback in Educational Settings. IEEE Access 9 (2021), 108190–108198. https://doi.org/10.1109/ACCESS.2021. 3100890
\citet{hahn:2021:SystematicReviewEffects} conducted a systematic review on the effects of automatic scoring and feedback tools, underscoring their critical role in scaling online education and reducing the time between submission and feedback. While their work primarily addresses graded feedback, the insights on scalability, bias reduction, and student engagement are highly relevant. This paper informs our understanding of the broader implications of automated feedback systems, reinforcing the potential benefits of DAFeeD in providing iterative, task-specific feedback that enhances student learning outcomes and engagement.

% A Systematic Literature Review of Automated Feedback Generation for Programming Exercises
% Hieke Keuning, Johan Jeuring, and Bastiaan Heeren. 2018. A Systematic Literature Review of Automated Feedback Generation for Programming Exercises. ACM Transactions on Computing Education 19, 1, Article 3 (Sept. 2018). https://doi.org/10.1145/3231711
% -> Explore potential of LLMs:
% Exploring the Potential of Large Language Models to Generate Formative Programming Feedback:
% Natalie Kiesler, Dominic Lohr, and Hieke Keuning. 2023. Exploring the Potential of Large Language Models to Generate Formative Programming Feedback. arXiv:2309.00029
\citet{keuning:2018:SystematicLiteratureReview} conducted a systematic review of 101 tools for automated feedback on programming exercises, highlighting that most tools focus on error identification rather than providing actionable guidance or adapting to specific instructional needs. \cite{kiesler:2023:ExploringPotentialLarge} extended this work by applying their feedback typology to explore the effectiveness of large language models (LLMs) like ChatGPT in generating formative programming feedback. They found that while LLMs can produce useful feedback, they often include misleading information for novices. Our DAFeeD system builds on these insights by leveraging advancements in LLMs to provide a integrated feedback process with iterative and customized feedback, facilitating continuous student improvement. To mitigate the risk of misleading information, our system informs students within the LMS that the feedback is AI-generated, emphasizing the need for critical evaluation. This approach ensures that students can benefit from immediate, tailored feedback while being aware of its limitations.

% Feedback-Generation for Programming Exercises With GPT-4:
% Imen Azaiz, Natalie Kiesler, and Sven Strickroth. 2024. Feedback-Generation for Programming Exercises With GPT-4. arXiv:2403.04449 [cs]
% Accepted at ITiCSE '24, first author is from LMU
The work by \citet{azaiz:2024:FeedbackGenerationProgrammingExercises} is relevant as it highlights the limitations of LLMs, advising against using GPT-4 Turbo for automatic feedback generation for programming education due to inconsistencies. In contrast, our research evaluates the integrated direct automatic feedback delivery process within an LMS, which we believe can already be beneficial for students. We anticipate that increasingly powerful LLMs and advanced prompting strategies will improve the feedback generation process over time without revealing solutions.


% > Human-AI Interaction in Education %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Grounded copilot: How programmers interact with code-generating models.
% \citet{barke:2023:GroundedCopilotHow}

% To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making.
% \citet{bucinca:2021:TrustThinkCognitive}


% > Large Language Models in Education %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation
% \citet{becker:2023:ProgrammingHardLeast}

% Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming
% \citet{kazemitabaar:2023:StudyingEffectAIa

% Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests
% \citet{hellas:2023:ExploringResponsesLargea}

% How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment
% \citet{kazemitabaar:2024:HowNovicesUse}

% Investigating the Potential of GPT-3 in Providing Feedback for Programming Assessments
% \citet{balse:2023:InvestigatingPotentialGPT3}

% Using Large Language Models to Enhance Programming Error Messages
% \citet{leinonen:2023:UsingLargeLanguagea}

% Next-Step Hint Generation for Introductory Programming Using Large Language Models
% \citet{roest:2024:NextStepHintGeneration}

% !! CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes
% \citet{liffiton:2024:CodeHelpUsingLarge}

% Design principles for generating and presenting automated formative feedback on code quality using software metrics
% \citet{vandenaker:2024:DesignPrinciplesGeneratingb}

% !!! Using GPT-4 to Provide Tiered, Formative Code Feedback
% \citet{nguyen:2024:UsingGPT4Provide}

% !!! AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course
% \citet{woodrow:2024:AITeachesArta}

% PyDex: Repairing Bugs in Introductory Python Assignments using LLMs
% \citet{zhang:2024:PyDexRepairingBugs}

%\newpage
\section{Approach: Direct Automated Feedback Delivery (DAFeeD)} % 1 page
\label{sec:approach:DAFeeD}

DAFeeD employs large language models to deliver automated feedback on student submissions, designed to complement traditional teaching methods and provide additional support.
Figure \ref{fig:DAFeeD-workflow} illustrates the continuous feedback workflow that DAFeeD facilitates, enabling students to receive feedback at any time, thereby eliminating the need to wait for responses from human tutors or course professors.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figures/DAFeeD-ActivityDiagram_alternative.pdf}
  \caption{Workflow of direct automated feedback delivery (DAFeeD) for students' submissions (UML Activity Diagram)}
  \label{fig:DAFeeD-workflow}
\end{figure}

The feedback process is designed to be exercise-independent, meaning that it can be applied to various exercise types, such as programming, text, or modeling exercises.
%todo: cite formative feedback
DAFeeD can provide formative feedback to the students, including feedback on issues or improvements, as well as positive feedback when the student completes the task correctly.
Once the student submits their solution, DAFeeD initiates a three-stage process to generate natural language feedback.

%todo: add learner profile (find reference) to generate individualized, personal feedback tailored to the student's learning progress and needs.
The first stage, called \textit{Formatting}, takes the student's submission and extracts the submission content, the problem statement, including learning objectives, and any possible grading instructions the instructor defines.
All of this gathered information represents the context.
During the prompt generation step, a predefined prompt template is filled with the prompt input data, resulting in the feedback prompt.
Depending on the exercise, adaptions need to be made to the prompt template to ensure that the feedback output of the LLM is tailored to the specific exercise type.
For programming exercises, the generated feedback needs to have metadata information about the file and line number of the code snippet to which the feedback refers.
For text exercises, the feedback needs to have metadata about the sentence or word range the feedback refers to.

In the second stage, called \textit{Predicting}, DAFeeD sends the feedback prompt to a large language model (LLM) and invokes it with the prompt. 
As a result, the LLM generates a response to that prompt including detailed feedback items for the student.

The final stage, \textit{Parsing}, takes the LLM response, which comes in the JSON format, and parses feedback items from it. 
In addition to the feedback text, the feedback object also contains reference information indicating the part of the submission it pertains to.
For programming exercises, this includes the file name and line number of the relevant code snippet to which the feedback refers.
For text exercises, the reference information includes only the sentence or word range the feedback refers to.

All of the feedback is then returned to the student for review.
If the student is satisfied with the feedback, the process concludes. 
Otherwise, the student can refine and resubmit their solution, initiating the DAFeeD process anew.

This iterative process is designed to motivate students to continuously learn and experiment with their solutions, resulting in improved performance.


\section{Reference Implementation: Athena} % 2 pages
\label{sec:reference-implementation}

We incorporated DAFeeD into a reference implementation named Athena, which is seamlessly integrated with the learning platform Artemis. 
Through Artemis, students can submit their solution and review the feedback.

When submitting their solutions on Artemis, students have the option to request direct automated feedback by clicking a newly added button.
This feedback request is then sent to Athena, provided the student has not reached their feedback request limit for the exercise.
Course instructors can customize the number of allowed feedback requests per exercise according to their preference.
A status visualization informs students about their feedback request state.
Once Athena generates the feedback and sends it back to Artemis, the student can review it in a modal window on Artemis, as depicted in Figure \ref{fig:Artemis-feedback-visualization}.

%todo: find better example for feedback including a positive feedback feedback item
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/artemis-feedback-alt_light_clarity-upscaler_anonymized.png}
  \caption{Visualization of the feedback how students see it in Artemis.}
  \label{fig:Artemis-feedback-visualization}
\end{figure}


\subsection{Feedback Generation}

The prompt design is crucial for guiding the large language model (LLM) in generating effective and contextually relevant feedback. 
In Figure \ref{fig:prompt-programming-exercise-generation}, we provide an example of a prompt used for generating feedback for programming exercises. 
This prompt incorporates specific instructions to ensure that the feedback is tailored to the student's submission.

The feedback generation process begins by identifying the differences between the student's submission repository and the provided template repository, the starting point. 
These differences are identified using a git diff, which highlights lines removed and added by the student. 
If the problem statement is too lengthy or complex, a separate LLM invocation is used to split the problem statement into relevant parts for each file. 
This ensures that the feedback is targeted and relevant to the specific context of the file being reviewed. 
Additionally, a summary of the student's solution across all files is generated using another LLM invocation. 
This summary provides a comprehensive overview of the submission, which is included in the prompt to offer context for the feedback.

In the provided prompt, several key components guide the AI in creating useful feedback. The \textit{Problem Statement} section contextualizes the student's task and helps the AI understand the exercise's objectives.
The \textit{Task Instructions} direct the AI to provide non-graded, constructive feedback focusing on educational aspects without offering direct solutions.
\textit{Style Guidelines} ensure the feedback is constructive, specific, balanced, clear, concise, actionable, educational, and contextual.
The \textit{File Path and Content} provide the specific file under review along with its content, aiding the AI in pinpointing specific lines of code for feedback.
Additionally, \textit{Summary and Diffs} between the template and submission offer additional context, helping the AI understand the student's changes and their overall approach.

The structure and content of this prompt are designed to emulate a human tutor's approach, ensuring that the feedback is both relevant and supportive of the student's learning process.
By providing such detailed instructions and contextual information, the LLM can generate feedback that is both meaningful and actionable for students.

% Prompt for generating feedback suggestions of programming exercises
\begin{figure}[!htbp]
  \begin{lstlisting}[style=prompt]
You are an AI tutor for programming assessment at a prestigious university.

# Problem statement
%*\highlight{\{problem\_statement\}}*)

# Task
Provide constructive, non-graded feedback on a student's programming submission as a human tutor would. The tutor is not familiar with the solution, so the feedback should focus solely on aspects from which the student can learn. This feedback must highlight incorrectly applied principles or inconsistencies without offering specific solutions or error corrections. Allow some flexibility for students to deviate from the problem statement, provided they complete all tasks. Ensure the feedback is balanced and comprehensive.

# Style
1. Constructive, 2. Specific, 3. Balanced, 4. Clear and Concise, 5. Actionable, 6. Educational, 7. Contextual

Feedback that contradicts the problem statement is strictly prohibited. Avoid mentioning aspects not explicitly covered in the template to submission diff, such as the exercise package name, as these are beyond the student's control.

In git diff, lines marked with '-' were removed and with '+' were added by the student.

The student will be reading your response, so use "you" instead of "them".

Path: %*\highlight{\{submission\_file\_path\}}*)

File (with line numbers <number>: <line>):
%*\highlight{\{submission\_file\_content\}}*)

Summary of other files in the solution:
%*\highlight{\{summary\}}*)

The template to submission diff (only as reference):
%*\highlight{\{template\_to\_submission\_diff\}}*)
  \end{lstlisting}
  \caption[Prompt for Generating Feedback for Programming Exercises]{Prompt for Generating Feedback for Programming Exercises. The highlighted sections are placeholders for the respective elements.}
  \label{fig:prompt-programming-exercise-generation}
  \end{figure}

\subsection{Architecture}

Athena is deployed in production alongside the learning platform Artemis, which serves up to more than 2000 students per course.
Consequently, the reference implementation must satisfy additional non-functional requirements such as performance, scalability, maintainability, and usability.
To meet these requirements and to support feedback generation for multiple exercise types while allowing for future extensibility, we adopted a modular architecture, as illustrated in Figure \ref{fig:Athena-architecture}.

%TODO: anonymize the figure (Artemis, Athena )
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figures/Athena-Architecture.pdf}
  \caption{Top Level Archietecture of the reference implementation Athena (UML Deployment Diagram)}
  \label{fig:Athena-architecture}
\end{figure}

The \textit{Module Manager} handles all incoming requests, verifies authorization, and forwards them to the appropriate modules.
The \textit{ProgrammingLLM} module manages programming exercises and executes the three-stage DAFeeD process, which includes formatting, predicting, and parsing. 
Similarly, the \textit{TextLLM} module is optimized for text exercises and follows the same process.

Athena's system design is independent of any specific learning management system (LMS) as it provides a REST API, documented using the OpenAPI standard\footnote{\url{https://www.openapis.org}}.
This independence allows Athena to be integrated with various LMS platforms, such as Moodle\footnote{\url{https://moodle.org}}.

Athena currently connects to OpenAI models hosted in a private Azure cloud to ensure that student data is not used for training models, maintaining privacy.
Additionally, the system can be configured to use open-source models like Llama\footnote{\url{https://llama.meta.com}} or Mistral\footnote{\url{https://mistral.ai}}, either self-hosted or cloud-based.

To meet performance and scalability requirements, Athena and its modules are deployed within a Kubernetes cluster\footnote{\url{https://kubernetes.io}}.
Kubernetes, in conjunction with Athena's modular architecture, allows the system to scale each module independently.
For example, additional instances of the programming module can be instantiated when a new programming exercise is released.
Furthermore, Kubernetes provides out-of-the-box load balancing and self-healing capabilities, ensuring that if a module crashes, it is automatically restarted.


\section{Evaluation} % 5 pages
\label{sec:evaluation}

In this section, we outline the methodology employed to validate the effectiveness of the proposed DAFeeD approach including the reference implementation Athena.
The conducted evaluation represents the treatment validation stage of the design science methodology proposed by Wieringa \cite{wieringa:2014:DesignScienceMethodologya}.
In this stage, the proposed solution — DAFeeD — is evaluated in a controlled environment, and the collected data is utilized for the refinement and improvement of the solution.

We begin by describing the research questions, followed by the study design and the results.
Subsequently, we outline the limitations of the evaluation and discuss the implications of the findings.


\subsection{Research Questions}

With this study, we want to answer the following research questions about direct automated feedback delivery:

\begin{enumerate}[label=\textbf{RQ\arabic*},ref=RQ\arabic*]
  \item \label{RQ1} How does the availability of direct automated feedback affect student engagement and motivation? 
  \item \label{RQ2} Do students feel more comfortable requesting automatic feedback than asking a human tutor or the course professor?
  \item \label{RQ3} How do students perceive the effectiveness of direct automated feedback?
  \item \label{RQ4} How do students perceive the usability and helpfulness of DAFeeD?
\end{enumerate}

\ref{RQ1} examines the influence of direct automated feedback on students' overall engagement with the course material and their motivation to complete exercises and improve their skills.
We aim to determine if the immediacy and convenience of automated feedback enhance students' commitment to their coursework.
\ref{RQ2} explores students' comfort levels with seeking feedback from an automated system compared to traditional sources such as human tutors or course professors. 
We want to assess whether students prefer the anonymity and immediacy of automated feedback over potentially intimidating interactions with instructors.
\ref{RQ3} seeks to understand students' views on the value and impact of the feedback provided by the DAFeeD system. 
We are interested in whether students find the feedback to be relevant and beneficial to their learning process.
\ref{RQ4} focuses on students' perceptions of the usability and helpfulness of the DAFeeD system. 
We aim to evaluate how intuitive and user-friendly students find the system, as well as how effective they consider it in assisting their learning and improving their performance.

\subsection{Study Design}
To gain comprehensive insights into students' perceptions of the newly introduced DAFeeD concept, we employed a survey-based approach.
Initially, study participants tested the new feedback feature on a sample exercise using the Artemis platform in a controlled environment at the university.
The study conductors were present throughout to answer questions and provide support as needed.
Following this hands-on experience, participants were asked to complete a survey hosted on the community version of the open-source survey tool LimeSurvey\footnote{\url{https://www.limesurvey.org}}.
This survey aimed to gather their opinions on direct automated feedback and collect feedback on their overall experience with the feature.

We invited students from current courses at the university to participate in the study via direct messages and conducted the study with a total of 20 participants.
The study uses a mixed methods approach, combining quantitative and qualitative data collection methods. 
The participants were a mix of undergraduate and graduate students from various disciplines, including computer science, information systems, and games engineering.
%todo: describe participants 
%todo: cite nielsen regarding number of participants
%todo: describe sample exercise

All survey questions, except for the introductory demographic queries and the five final, voluntary free-text responses (Q18 - Q22), employ a 5-point Likert scale \cite{allen:2007:LikertScalesData} ranging from "strongly agree" to "strongly disagree" and are mandatory.
The following list shows the mapping from the asked questions and statements to the research questions:

\begin{enumerate}[label=\textbf{RQ\arabic*}]
  \item \textbf{Engagement and Motivation}
    \begin{enumerate}[label=\textbf{Q\arabic*},resume,ref=Q\arabic*]
      \item \label{Q1} The direct automated feedback keeps me more engaged in the learning process.
      \item \label{Q2} The direct automatic feedback motivates me to repeatedly improve my code.
      \item \label{Q3} The direct automated feedback makes me feel more motivated to complete my programming assignments.
      \item \label{Q4} The direct automated feedback encourages me to experiment more with my coding solutions.
    \end{enumerate}
  \item \textbf{Comfort with Feedback Source}
  \begin{enumerate}[label=\textbf{Q\arabic*},resume,ref=Q\arabic*]
    \item \label{Q5} I feel more comfortable requesting direct automated feedback than feedback from a human tutor.
    \item \label{Q6} I am likely to request feedback more frequently when using direct automated feedback than feedback from my course professor.
    \item \label{Q7} I find receiving direct automated feedback less intimidating than receiving feedback from a human tutor.
    \item \label{Q8} I feel that requesting direct automated feedback is more convenient than arranging a meeting with a human tutor.
  \end{enumerate}
  \item \textbf{Perceived Effectiveness}
    \begin{enumerate}[label=\textbf{Q\arabic*},resume,ref=Q\arabic*]
      \item \label{Q9} The direct automated feedback helps me understand my mistakes.
      \item \label{Q10} The direct automated feedback is more effective than one-time feedback.
      \item \label{Q11} The direct automated feedback has significantly improved the quality of my programming assignment.
      \item \label{Q12} The direct automated feedback is a helpful addition to the automatic test case results.
      \item \label{Q13} I feel that having access to direct automated feedback continuously helps me more than arranging a meeting with a human tutor.
    \end{enumerate}
  \item \textbf{Usability and Helpfulness}
    \begin{enumerate}[label=\textbf{Q\arabic*},resume,ref=Q\arabic*]
      \item \label{Q14} It is easy to receive direct automated feedback  on my programming assignments.
      \item \label{Q15} I would rather use the direct automated feedback integrated into Artemis than use an external AI tool for getting feedback.
      \item \label{Q16} I find the direct automated feedback helpful in improving my programming skills.
      \item \label{Q17} I am satisfied with the overall performance of the direct automated feedback.
      \item \label{Q18} Are there any improvements that you would suggest for direct automated feedback?
      \item \label{Q19} How did you find the feedback?
      \item \label{Q20} What kind of feedback would you like to receive?
      \item \label{Q21} Was there anything you particularly liked about the direct automated feedback process?
      \item \label{Q22} What difficulties did you encounter when using the direct automated feedback process?
    \end{enumerate}
\end{enumerate}

%todo: Describe free text questions

% %Question Group 1
% The survey commences with a series of introductory demographic inquiries, including the student's current study program, ongoing degree pursued, and academic semester. 
% Additionally, respondents are asked to provide information regarding the hardware employed, including details on the operating system and web browser used.
% The final question in this group asks participants to describe their programming experience.

% %Question Group 2
% Following the demographic section, the survey explores participants' opinions on the impact of direct automated feedback on student engagement and motivation.
% %Question Group 3
% Subsequently, the survey queries participants on their comfort with the feedback source, specifically whether they prefer receiving feedback from a human tutor or from the DAFeeD feature.
% %Question Group 4
% The survey then investigates aspects related to participants' perceived effectiveness of the DAFeeD feature.
% %Question Group 5
% In the final question group, participants assess the overall usability and helpfulness of the DAFeeD feature.


\subsection{Results}

In the following paragraphs, we present the results. 
The answers to each of the likert scale questions are visualized in Figure \ref{fig:Survey-Results}.

\begin{figure}[htpb]
  \centering
  \resizebox{0.8\textwidth}{!}{
    \input{figures/likert_responses.pgf}
  }
  \caption[Distribution of Likert scale responses]{Distribution of Likert scale responses.}
  \label{fig:Survey-Results}
\end{figure}


%todo make this a bit shorter and simplify sentences
90\% of students indicated that the direct automated feedback from Artemis keeps them more engaged in the learning process, with 10\% neutral (\ref{Q1}).
For \ref{Q2}, 85\% of students stated that the direct automatic feedback from Artemis motivates them to repeatedly improve their code, with 10\% neutral and 5\% disagreeing.
For \ref{Q3}, 85\% of students mentioned that the direct automated feedback from Artemis makes them feel more motivated to complete their programming assignments, with 10\% neutral and 5\% disagreeing.
The encouragement to experiment more with coding solutions (\ref{Q4}) received a positive response, with 70\% agreeing, 20\% neutral, and 10\% disagreeing.


Comfort levels in requesting feedback showed that 90\% of students feel more comfortable requesting direct automated feedback from Artemis than feedback from a human tutor, with 10\% neutral (\ref{Q5}).
For \ref{Q6}, 100\% of students noted that they are likely to request feedback more frequently when using direct automated feedback from Artemis than feedback from their course professor.
Receiving feedback from Artemis was found to be less intimidating than from a human tutor by 80\% of students, with 10\% neutral and 10\% disagreeing (\ref{Q7}).
Convenience in requesting feedback showed that 100\% of students feel that requesting direct automated feedback from Artemis is more convenient than arranging a meeting with a human tutor (\ref{Q8}).


In terms of understanding mistakes, 90\% of students believed that the direct automated feedback provided by Artemis helps them understand their mistakes, with 10\% neutral (\ref{Q9}).
The effectiveness of the feedback was highlighted by 95\% of students who found that the direct automated feedback from Artemis is more effective than one-time feedback, with 5\% neutral (\ref{Q10}).
Regarding the quality of assignments, 70\% of students observed that the direct automated feedback has significantly improved the quality of their programming assignments, with 20\% neutral and 10\% disagreeing (\ref{Q11}).
For \ref{Q12}, 95\% of students felt that the direct automated feedback is a helpful addition to the automatic test case results, with 5\% neutral.
Continuous access to feedback was found to be more beneficial than arranging meetings with a tutor by 75\% of students, with 15\% neutral and 10\% disagreeing (\ref{Q13}).


Ease of receiving feedback was highly rated, with 100\% of students confirming that it is easy to receive direct automated feedback from Artemis on their programming assignments (\ref{Q14}).
Furthermore, 70\% of students preferred using the direct automated feedback integrated into Artemis over using an external AI tool for getting feedback, with 25\% neutral and 5\% disagreeing (\ref{Q15}).
In terms of skill improvement, 70\% of students agreed that they find the direct automated feedback from Artemis helpful in improving their programming skills, with 25\% neutral and 5\% disagreeing (\ref{Q16}).
Lastly, 80\% of students were satisfied with the overall performance of the direct automated feedback system, with 10\% neutral and 10\% disagreeing (\ref{Q17}).


% free text responses
% todo: add Question numbers 
The responses to the voluntary free text questions highlight several themes.
Many students appreciated the immediate availability of feedback, which allowed for prompt corrections without waiting for manual review. 
However, some respondents suggested improvements such as better categorization of feedback, more detailed explanations of errors, and prioritization of critical issues. 
The feedback was generally found to be relevant and useful in addressing obvious mistakes and improving code quality. 
Students expressed a preference for feedback that clearly identified mistakes and provided specific guidance on how to correct them, along with suggestions for improvement. 
Some challenges included understanding certain automated feedback messages and occasional false positives or negatives in error detection.


\subsection{Findings}

The responses to \ref{RQ1} show that the availability of direct automated feedback can significantly enhance student engagement and motivation.
Students reported feeling more engaged in the learning process.
The majority of participants also stated they are motivated to repeatedly improve their code and complete their programming assignments.
Additionally, the feedback encouraged a notable percentage of participants to experiment more with their coding solutions.
These findings suggest that direct automated feedback is highly effective in boosting both engagement and motivation among students.

\begin{findingsBox}
	\textbf{Main Findings for RQ1:} The availability of direct automated feedback significantly enhances student engagement and motivation. 
  Students feel more engaged in the learning process, motivated to improve their code, and encouraged to experiment more with their coding solutions, without having to wait for manual feedback.
\end{findingsBox}

The responses to \ref{RQ2} reveal a strong level of comfort for requesting automated feedback compared to traditional human feedback channels.
Students stated they feel more comfortable requesting automated feedback than from human tutors and were likely to request automated feedback more frequently than from their course professors.
Requesting automated feedback was perceived as less intimidating for most of the participants and all of them stated it is more convenient than arranging meetings with a human tutor.
These findings highlight the effectiveness of automated feedback in providing a more comfortable and accessible feedback mechanism for students.

\begin{findingsBox}
	\textbf{Main Findings for RQ2:} Students feel more comfortable requesting automated feedback than from human. 
  They are likely to request automated feedback more frequently and find it less intimidating and more convenient than arranging meetings with a human tutor.
  % TODO: Does this fit here or in the discussion
  % Nevertheless, DAFeeD's role should be seen as a complementary tool rather than a complete substitute for humans.
\end{findingsBox}

The responses to \ref{RQ3} indicate that students think automated feedback is highly effective in helping them understand and improve their programming assignments. 
Students reported that the feedback helped them understand their mistakes and found it more effective than receiving only one-time feedback for their submission.
The majority reported that the feedback significantly improved the quality of their programming assignments, and all participants stated that automatic feedback is a helpful addition to automatic test case results generated by Artemis.
In addition, most participants saw continuous access to autamted feedback as more beneficial than arranging meetings with a tutor. 
These findings suggest that automated feedback not only aids in error identification but also significantly enhances the overall quality of student assignments.

\begin{findingsBox}
	\textbf{Main Findings for RQ3:} Students perceive automated feedback as highly effective in helping them understand and improve their programming assignments. 
  The feedback helps them understand their mistakes, improves the quality of their assignments, and is a helpful addition to automatic test case results.
\end{findingsBox}

The responses to \ref{RQ4} demonstrate the ease of receiving feedback and overall satisfaction with DAFeeD's feedback process and its reference implementation. 
Students found it easy to receive feedback on their programming assignments.
A large number of participants preferred using the feedback integrated into Artemis than copy their submission and relevant context information over to an external AI tool. 
Most participants also deemed the feedback helpful in improving their programming skills. 
In regards to the overall performance of the direct automated feedback, the majority of students expressed high satisfaction with the system.
%These findings underscore the high level of user satisfaction and the perceived effectiveness of Artemis' automated feedback in enhancing programming skills.
%TODO: free text answers including improvements

\begin{findingsBox}
	\textbf{Main Findings for RQ4:} Students find it easy to receive feedback on their programming assignments and are satisfied with the overall performance of DAFeeD and its reference implementation. 
  There are some suggestions for improvements, such as better categorization of feedback, more detailed explanations of errors, and prioritization of critical issues.
\end{findingsBox}


\subsection{Discussion}

Overall, the direct automated feedback system was positively received by students, indicating its effectiveness in enhancing their programming skills, despite some areas for improvement.

% complement, not substitute
% validation study

% enables interactive learning
% feedback quality can be improved



\subsection{Limitations}
We follow the categorization framework proposed by Runeson and Höst \cite{runeson:2009:GuidelinesConductingReporting} to outline the limitations of the conducted evaluation, addressing potential threats to internal, external, and construct validity:

% internal validity
% copilot: Internal validity threats include the potential for confounding variables, such as the students' prior experience with automated feedback systems, which could influence their perceptions of the DAFeeD system.
\textit{Internal Validity}: This study may be compromised by using self-reported survey data, which can introduce biases. 
Participants' perceptions may be influenced by their individual attitudes or varying levels of familiarity with programming concepts, leading to potential inaccuracies.
Additionally, the participants' perceived effectiveness does not necessarily correspond to objective effectiveness.

% external validity
\textit{External Validity}: Threats to external validity arise from the specific context of this study. 
Conducting the research exclusively at a single university and with students from computer science, information systems, and similar programs restricts the diversity of the sample. 
This narrow focus may limit the applicability of the findings to other educational settings or student populations. 
In addition, the small sample size may also limit the generalizability of the findings.

% construct validity
\textit{Construct Validity}: 
The survey questions designed to evaluate 'perceived effectiveness' and 'comfort with feedback source' may not fully encompass the breadth of these constructs.
Factors such as prior experiences and personal preferences, which the survey does not account for, could influence participants' responses and perceptions.


\section{Conclusion \& Future Work} % 1 page
\label{sec:conclusion}

% todo: summarize the contributions and outcome

%future work
% 
Future work includes enhancing the visualization of feedback, such as grouping and color coding the feedback items to make it easier to differentiate between critical feedback items, suggestions for improvement, and positive feedback.
A high priority will be on further improving the overall quality of the feedback provided. 
We also aim to extend the implementation to support direct automated feedback for the remaining exercise types of Artemis.
Another crucial step is to test direct automated feedback in a real-world setting by utilizing this feature in an actual course. 
This will allow us to collect comprehensive data to thoroughly evaluate the impact on student performance and motivation.

% todo: AI guidelines?

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
\endinput
%%